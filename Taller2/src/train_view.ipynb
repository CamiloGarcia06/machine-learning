{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99466a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers, regularizers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2f18e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "path = Path(\"data/public_dataset.csv\")\n",
    "df = pd.read_csv(path)\n",
    "y = df[\"y\"].astype(int)          # Series\n",
    "X = df.drop(columns=[\"y\"])       # DataFrame\n",
    "\n",
    "# Ratios deseados\n",
    "train_ratio = 0.60\n",
    "val_ratio   = 0.20\n",
    "test_ratio  = 0.20\n",
    "assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6acd5fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "  train: (460, 1280)\n",
      "  val  : (154, 1280)\n",
      "  test : (154, 1280)\n",
      "\n",
      "Distribución (proporciones):\n",
      "  train:\n",
      " y\n",
      "0    0.785\n",
      "1    0.215\n",
      "Name: proportion, dtype: float64\n",
      "  val:\n",
      " y\n",
      "0    0.786\n",
      "1    0.214\n",
      "Name: proportion, dtype: float64\n",
      "  test:\n",
      " y\n",
      "0    0.786\n",
      "1    0.214\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1) separar TEST\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=test_ratio, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "# 2) separar TRAIN/VAL desde el resto\n",
    "rel_val = val_ratio / (1.0 - test_ratio)   # proporción de val dentro de temp\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=rel_val, stratify=y_temp, random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"  train:\", X_train.shape)\n",
    "print(\"  val  :\", X_val.shape)\n",
    "print(\"  test :\", X_test.shape)\n",
    "\n",
    "print(\"\\nDistribución (proporciones):\")\n",
    "print(\"  train:\\n\", y_train.value_counts(normalize=True).round(3))\n",
    "print(\"  val:\\n\",   y_val.value_counts(normalize=True).round(3))\n",
    "print(\"  test:\\n\",  y_test.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a7a4ec",
   "metadata": {},
   "source": [
    "# 1) Límites de outliers con IQR (calculados solo en TRAIN)\n",
    "\n",
    "Para cada columna calculamos umbrales extremos usando ±3×IQR (más conservador que ±1.5×IQR).\n",
    "\n",
    "Si una columna no tiene variación (IQR=0), no clippeamos.\n",
    "\n",
    "¿Por qué solo en TRAIN? Para evitar data leakage: los límites se aprenden con train y se aplican tal cual a val/test/quiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2af68979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': (-0.1481315415, 0.197508722),\n",
       " '1': (-3.3103462390000002, 4.46047177925),\n",
       " '2': (-0.186483387, 0.24864451599999998),\n",
       " '3': (-1.03108047, 1.37477396),\n",
       " '4': (-0.6830287275, 0.9107049700000001),\n",
       " '5': (-0.16766023874999997, 0.22354698499999998),\n",
       " '6': (-2.9120271, 6.1332583624999994),\n",
       " '7': (-inf, inf),\n",
       " '8': (-0.6473594715, 0.8673045779999999),\n",
       " '9': (-3.4003634250000014, 9.719917525000003),\n",
       " '10': (-0.0654800295, 0.087306706),\n",
       " '11': (-3.4158384500000025, 9.679822100000003),\n",
       " '12': (-0.3520149135, 0.469353218),\n",
       " '13': (-2.3079510565000003, 3.109890798625),\n",
       " '14': (-3.6297810750000004, 8.625659850000002),\n",
       " '15': (-1.5821722349999998, 2.3307142075),\n",
       " '16': (-2.493656155, 3.4825515825),\n",
       " '17': (-inf, inf),\n",
       " '18': (-0.3338106825, 0.44508091),\n",
       " '19': (-0.0471962205, 0.062928294),\n",
       " '20': (-inf, inf),\n",
       " '21': (-0.23161106250000002, 0.30881475),\n",
       " '22': (-1.7897680900000004, 3.2053986425000005),\n",
       " '23': (-inf, inf),\n",
       " '24': (-0.391086782, 0.5514052527500001),\n",
       " '25': (-inf, inf),\n",
       " '26': (-0.23589263700000002, 0.314523516),\n",
       " '27': (-3.7925535749999995, 8.4742284),\n",
       " '28': (-0.15671238075, 0.208949841),\n",
       " '29': (-0.9035064274999997, 1.9422373474999997),\n",
       " '30': (-inf, inf),\n",
       " '31': (-1.12479292, 1.839147815),\n",
       " '32': (-inf, inf),\n",
       " '33': (-0.0002744100675, 0.00036588009),\n",
       " '34': (-0.6870891805, 0.921105511625),\n",
       " '35': (-0.2785191675, 0.37135889),\n",
       " '36': (-inf, inf),\n",
       " '37': (-2.1788717710000003, 2.9996966782500003),\n",
       " '38': (-5.627344699999999, 8.732726374999999),\n",
       " '39': (-inf, inf),\n",
       " '40': (-0.33495601799999997, 0.446608024),\n",
       " '41': (-0.248844045, 0.33179206),\n",
       " '42': (-0.7647400755, 1.05800844475),\n",
       " '43': (-inf, inf),\n",
       " '44': (-1.0969062374999998, 1.6756474274999997),\n",
       " '45': (-inf, inf),\n",
       " '46': (-0.004962575475, 0.0066167673),\n",
       " '47': (-0.0016485111900000002, 0.00219801492),\n",
       " '48': (-2.2651621250000007, 3.8470842250000006),\n",
       " '49': (-inf, inf),\n",
       " '50': (-inf, inf),\n",
       " '51': (-inf, inf),\n",
       " '52': (-inf, inf),\n",
       " '53': (-0.032642053875, 0.0435227385),\n",
       " '54': (-6.896600975, 9.766111787500002),\n",
       " '55': (-1.2242769454999998, 1.6648389747499999),\n",
       " '56': (-inf, inf),\n",
       " '57': (-inf, inf),\n",
       " '58': (-2.42726416, 3.6016780075),\n",
       " '59': (-inf, inf),\n",
       " '60': (-3.383267949999999, 5.900954549999999),\n",
       " '61': (-0.7768014925, 1.0621605887499999),\n",
       " '62': (-0.070899768, 0.09453302400000001),\n",
       " '63': (-inf, inf),\n",
       " '64': (-inf, inf),\n",
       " '65': (-0.66987198, 0.89316264),\n",
       " '66': (-inf, inf),\n",
       " '67': (-inf, inf),\n",
       " '68': (-3.5470986400000015, 6.675071517500001),\n",
       " '69': (-0.052746324, 0.070328432),\n",
       " '70': (-0.08490803025, 0.11321070700000001),\n",
       " '71': (-4.5235121750000005, 7.3304668500000005),\n",
       " '72': (-inf, inf),\n",
       " '73': (-1.4391404175, 2.30922737),\n",
       " '74': (-1.056070934, 1.43135034675),\n",
       " '75': (-0.1164992325, 0.15533231),\n",
       " '76': (-1.31923936, 1.937695895),\n",
       " '77': (-inf, inf),\n",
       " '78': (-1.1799614615, 1.6408689705000001),\n",
       " '79': (-inf, inf),\n",
       " '80': (-1.7204973150000005, 4.00230113),\n",
       " '81': (-0.27106620000000003, 0.3614216),\n",
       " '82': (-inf, inf),\n",
       " '83': (-5.926952985000002, 8.968940457500002),\n",
       " '84': (-0.0558513915, 0.074468522),\n",
       " '85': (-0.4884825075, 0.65131001),\n",
       " '86': (-0.9291681845, 1.30731455525),\n",
       " '87': (-0.0220753239, 0.0294337652),\n",
       " '88': (-inf, inf),\n",
       " '89': (-0.008800002975, 0.0117333373),\n",
       " '90': (-3.6196422349999997, 5.25928597),\n",
       " '91': (-inf, inf),\n",
       " '92': (-0.6841887175000001, 0.9331395825000001),\n",
       " '93': (-1.6258400499999999, 2.8723968749999997),\n",
       " '94': (-0.05396219925, 0.071949599),\n",
       " '95': (-0.109963812, 0.146618416),\n",
       " '96': (-inf, inf),\n",
       " '97': (-3.35047605, 7.3145682),\n",
       " '98': (-3.3549830700000003, 6.1908983275),\n",
       " '99': (-3.4987076250000015, 11.641224700000002),\n",
       " '100': (-0.05711462099999999, 0.07615282799999999),\n",
       " '101': (-0.18813430875, 0.250845745),\n",
       " '102': (-0.4504303575, 0.60057381),\n",
       " '103': (-0.11374796625000001, 0.15166395500000002),\n",
       " '104': (-inf, inf),\n",
       " '105': (-0.09972154724999999, 0.132962063),\n",
       " '106': (-1.01123083, 1.5166375374999999),\n",
       " '107': (-inf, inf),\n",
       " '108': (-1.7542454999999988, 6.8321466499999985),\n",
       " '109': (-2.3719412500000003, 3.8813096000000007),\n",
       " '110': (-inf, inf),\n",
       " '111': (-0.23923212, 0.31897616),\n",
       " '112': (-0.23658973125000002, 0.315452975),\n",
       " '113': (-0.006549611399999999, 0.008732815199999999),\n",
       " '114': (-1.8519491049999999, 3.7892597725),\n",
       " '115': (-2.77643885, 5.6766261375),\n",
       " '116': (-1.8393382750000007, 8.429596975),\n",
       " '117': (-0.150493092, 0.20065745599999998),\n",
       " '118': (-inf, inf),\n",
       " '119': (-1.4505436156, 1.9478945667),\n",
       " '120': (-inf, inf),\n",
       " '121': (-1.869547299, 2.542385968),\n",
       " '122': (-inf, inf),\n",
       " '123': (-inf, inf),\n",
       " '124': (-inf, inf),\n",
       " '125': (-0.410881005, 0.5478413400000001),\n",
       " '126': (-0.6405854824999999, 0.9564450749999999),\n",
       " '127': (-0.1960780425, 0.26143739),\n",
       " '128': (-2.0564299475, 2.9903155675),\n",
       " '129': (-inf, inf),\n",
       " '130': (-2.4300494350000004, 3.6509232075000004),\n",
       " '131': (-inf, inf),\n",
       " '132': (-0.07393621125, 0.098581615),\n",
       " '133': (-inf, inf),\n",
       " '134': (-0.045754077375, 0.0610054365),\n",
       " '135': (-1.531786425, 2.5729092749999998),\n",
       " '136': (-inf, inf),\n",
       " '137': (-0.19410747750000001, 0.25880997),\n",
       " '138': (-0.097153095, 0.12953746),\n",
       " '139': (-0.041783773875, 0.055711698500000004),\n",
       " '140': (-inf, inf),\n",
       " '141': (-0.6502381119999999, 0.9282587764999999),\n",
       " '142': (-inf, inf),\n",
       " '143': (-inf, inf),\n",
       " '144': (-inf, inf),\n",
       " '145': (-2.0512297999999998, 6.21405835),\n",
       " '146': (-0.822519575, 1.2268334125),\n",
       " '147': (-3.8387224000000004, 9.245282275000001),\n",
       " '148': (-1.095424766, 1.51448325325),\n",
       " '149': (-5.123931489999999, 6.871700479999999),\n",
       " '150': (-0.10308971249999999, 0.13745295),\n",
       " '151': (-0.7533026443, 1.022689929475),\n",
       " '152': (-inf, inf),\n",
       " '153': (-inf, inf),\n",
       " '154': (-inf, inf),\n",
       " '155': (-inf, inf),\n",
       " '156': (-inf, inf),\n",
       " '157': (-inf, inf),\n",
       " '158': (-4.063186815, 5.7983900675),\n",
       " '159': (-0.10276202475000001, 0.137016033),\n",
       " '160': (-0.34544415, 0.4605922),\n",
       " '161': (-0.0826555125, 0.11020735),\n",
       " '162': (-inf, inf),\n",
       " '163': (-2.12172855, 3.438390525),\n",
       " '164': (-0.038813221125, 0.0517509615),\n",
       " '165': (-inf, inf),\n",
       " '166': (-0.852668475, 1.1368913),\n",
       " '167': (-inf, inf),\n",
       " '168': (-inf, inf),\n",
       " '169': (-0.03887083575, 0.051827781),\n",
       " '170': (-0.59785044035, 0.7976059608875),\n",
       " '171': (-inf, inf),\n",
       " '172': (-inf, inf),\n",
       " '173': (-0.5206015465, 0.7040025755),\n",
       " '174': (-inf, inf),\n",
       " '175': (-0.127958868, 0.170611824),\n",
       " '176': (-inf, inf),\n",
       " '177': (-0.7171742025, 0.95623227),\n",
       " '178': (-2.5089683450000004, 3.98570239),\n",
       " '179': (-inf, inf),\n",
       " '180': (-0.0725409315, 0.096721242),\n",
       " '181': (-inf, inf),\n",
       " '182': (-0.038836015125, 0.0517813535),\n",
       " '183': (-1.302511741, 1.79891072825),\n",
       " '184': (-0.31916629650000006, 0.42555506200000004),\n",
       " '185': (-0.6019917512999999, 0.817460025975),\n",
       " '186': (-0.960020505, 1.5097939925000001),\n",
       " '187': (-1.3742124745000002, 1.8702555215),\n",
       " '188': (-0.19550094075000002, 0.260667921),\n",
       " '189': (-0.1931951025, 0.25759347),\n",
       " '190': (-2.7769002, 7.3632542999999995),\n",
       " '191': (-inf, inf),\n",
       " '192': (-inf, inf),\n",
       " '193': (-0.6563074545, 0.92552833775),\n",
       " '194': (-0.03714791625, 0.049530555000000004),\n",
       " '195': (-0.12310787999999999, 0.16414383999999999),\n",
       " '196': (-3.005239210000001, 6.059245695000001),\n",
       " '197': (-0.73701507, 0.98268676),\n",
       " '198': (-2.49841315, 4.08792815),\n",
       " '199': (-0.006534627449999999, 0.0087128366),\n",
       " '200': (-1.167894889, 1.6026467117499998),\n",
       " '201': (-2.614339215, 3.75192456125),\n",
       " '202': (-0.44397704250000003, 0.59196939),\n",
       " '203': (-inf, inf),\n",
       " '204': (-inf, inf),\n",
       " '205': (-0.02263752285, 0.0301833638),\n",
       " '206': (-inf, inf),\n",
       " '207': (-1.0078374711, 1.3486555770750002),\n",
       " '208': (-inf, inf),\n",
       " '209': (-0.2367767025, 0.31570227),\n",
       " '210': (-inf, inf),\n",
       " '211': (-2.0452365450000003, 3.533924315),\n",
       " '212': (-inf, inf),\n",
       " '213': (-inf, inf),\n",
       " '214': (-0.8994909174999999, 1.9713436399999997),\n",
       " '215': (-0.39047837250000006, 0.5206378300000001),\n",
       " '216': (-1.5990023025, 2.5201339425000002),\n",
       " '217': (-inf, inf),\n",
       " '218': (-inf, inf),\n",
       " '219': (-3.1737143100000003, 6.11605457),\n",
       " '220': (-0.012238698, 0.016318264),\n",
       " '221': (-0.07935726750000001, 0.10580969000000001),\n",
       " '222': (-1.2894994665000001, 1.7880008580000002),\n",
       " '223': (-0.064650726, 0.086200968),\n",
       " '224': (-2.05859401, 3.1336932825000003),\n",
       " '225': (-0.879102688, 1.26455043725),\n",
       " '226': (-2.5412001949999996, 4.75589734),\n",
       " '227': (-0.39920648999999997, 0.53227532),\n",
       " '228': (-inf, inf),\n",
       " '229': (-1.3902227925, 2.6271798275),\n",
       " '230': (-2.8836275000000007, 7.747563350000001),\n",
       " '231': (-1.0492415295, 1.443709249),\n",
       " '232': (-4.253013025, 6.9516943),\n",
       " '233': (-inf, inf),\n",
       " '234': (-0.4718607955, 0.6365983534999999),\n",
       " '235': (-0.7030128685, 0.966297350125),\n",
       " '236': (-4.374778249999999, 6.112454274999999),\n",
       " '237': (-0.155308698, 0.20707826399999998),\n",
       " '238': (-1.2992550116000001, 1.744398886825),\n",
       " '239': (-0.9378558075000001, 1.25047441),\n",
       " '240': (-0.038518834125, 0.0513584455),\n",
       " '241': (-0.15499741275, 0.206663217),\n",
       " '242': (-inf, inf),\n",
       " '243': (-inf, inf),\n",
       " '244': (-inf, inf),\n",
       " '245': (-2.127092795, 3.5995382025),\n",
       " '246': (-inf, inf),\n",
       " '247': (-inf, inf),\n",
       " '248': (-2.9097074000000003, 7.551604300000001),\n",
       " '249': (-inf, inf),\n",
       " '250': (-0.6379862535, 0.887631312),\n",
       " '251': (-1.28446265, 2.1660843100000005),\n",
       " '252': (-inf, inf),\n",
       " '253': (-0.7869972512000001, 1.06262679465),\n",
       " '254': (-inf, inf),\n",
       " '255': (-0.8362599500000005, 8.13013175),\n",
       " '256': (-0.9377883349999998, 1.7323533699999998),\n",
       " '257': (-0.31004704650000003, 0.413396062),\n",
       " '258': (-0.301756419949, 0.40238811808674996),\n",
       " '259': (-0.7471442024999999, 0.9961922699999999),\n",
       " '260': (-2.773473475, 4.773511075),\n",
       " '261': (-inf, inf),\n",
       " '262': (-inf, inf),\n",
       " '263': (-0.010686483075, 0.0142486441),\n",
       " '264': (-inf, inf),\n",
       " '265': (-0.10951721625000001, 0.146022955),\n",
       " '266': (-inf, inf),\n",
       " '267': (-inf, inf),\n",
       " '268': (-2.1737393175, 3.2414179649999997),\n",
       " '269': (-1.8823235549999997, 3.1290951474999997),\n",
       " '270': (-0.06715836975, 0.089544493),\n",
       " '271': (-0.8053350646999999, 1.079597669775),\n",
       " '272': (-1.6457459299999995, 4.134009485),\n",
       " '273': (-0.22147717200000003, 0.29530289600000004),\n",
       " '274': (-4.929334722, 6.597777504),\n",
       " '275': (-0.0020431717575, 0.00272422901),\n",
       " '276': (-inf, inf),\n",
       " '277': (-2.132425458, 2.8761598509999997),\n",
       " '278': (-2.232029105, 5.121157785),\n",
       " '279': (-inf, inf),\n",
       " '280': (-inf, inf),\n",
       " '281': (-inf, inf),\n",
       " '282': (-inf, inf),\n",
       " '283': (-3.5500793649999993, 5.154442142499999),\n",
       " '284': (-4.622939145, 8.37448274),\n",
       " '285': (-1.2896509289999998, 1.8305405929999998),\n",
       " '286': (-2.5252564875, 3.36700865),\n",
       " '287': (-1.4806803250000007, 3.6922798625000004),\n",
       " '288': (-inf, inf),\n",
       " '289': (-0.443583685822, 0.5920700649915),\n",
       " '290': (-1.7293935009999997, 2.5554080269999995),\n",
       " '291': (-1.480908355, 2.1298017199999997),\n",
       " '292': (-0.3614540175, 0.48193869),\n",
       " '293': (-inf, inf),\n",
       " '294': (-2.0596082350000002, 3.5123096200000004),\n",
       " '295': (-inf, inf),\n",
       " '296': (-0.12796563449999998, 0.170620846),\n",
       " '297': (-inf, inf),\n",
       " '298': (-inf, inf),\n",
       " '299': (-1.5074662499999998, 3.2092950124999997),\n",
       " '300': (-inf, inf),\n",
       " '301': (-1.5345577750000001, 2.9832953375000004),\n",
       " '302': (-3.2613797770000006, 4.431928051500001),\n",
       " '303': (-1.4311423700000003, 2.4341156175000003),\n",
       " '304': (-inf, inf),\n",
       " '305': (-inf, inf),\n",
       " '306': (-inf, inf),\n",
       " '307': (-0.04893087824999999, 0.06524117099999999),\n",
       " '308': (-0.7651552314000001, 1.02413435355),\n",
       " '309': (-inf, inf),\n",
       " '310': (-0.37680813, 0.50241084),\n",
       " '311': (-0.8142601799999999, 1.3334278099999999),\n",
       " '312': (-1.9387740599999999, 2.6748652324999997),\n",
       " '313': (-3.2043475050000003, 5.301023547500001),\n",
       " '314': (-inf, inf),\n",
       " '315': (-0.007673606925, 0.0102314759),\n",
       " '316': (-5.82424415, 7.9346941375),\n",
       " '317': (-0.4203499125, 0.56046655),\n",
       " '318': (-1.625614605, 3.35349516),\n",
       " '319': (-2.928896568, 4.00811479475),\n",
       " '320': (-inf, inf),\n",
       " '321': (-2.8060846349999995, 4.048461682499999),\n",
       " '322': (-0.5199229123, 0.701842347975),\n",
       " '323': (-inf, inf),\n",
       " '324': (-0.14190327225000002, 0.18920436300000001),\n",
       " '325': (-1.733309862, 2.3526032839999997),\n",
       " '326': (-inf, inf),\n",
       " '327': (-1.2278182940000002, 1.6971813517500003),\n",
       " '328': (-inf, inf),\n",
       " '329': (-0.19366767375, 0.258223565),\n",
       " '330': (-0.9351060924999999, 1.3359059025),\n",
       " '331': (-2.6216757250000002, 4.06251385),\n",
       " '332': (-3.757170559999999, 6.028300344999999),\n",
       " '333': (-0.38513372999999995, 0.5135116399999999),\n",
       " '334': (-inf, inf),\n",
       " '335': (-1.6957022650000007, 3.0420945925000007),\n",
       " '336': (-2.209696365000001, 5.17011933),\n",
       " '337': (-inf, inf),\n",
       " '338': (-1.82874708, 2.74298792),\n",
       " '339': (-1.5417264675, 2.9054619775),\n",
       " '340': (-0.07486627725, 0.099821703),\n",
       " '341': (-0.15061624199999998, 0.200821656),\n",
       " '342': (-0.16664710424999998, 0.222196139),\n",
       " '343': (-0.61905984, 0.82541312),\n",
       " '344': (-inf, inf),\n",
       " '345': (-2.3559301649999997, 5.0556915175),\n",
       " '346': (-0.265788289, 0.37621850300000004),\n",
       " '347': (-inf, inf),\n",
       " '348': (-inf, inf),\n",
       " '349': (-0.1568172735, 0.209089698),\n",
       " '350': (-1.194908958, 1.6706970597500002),\n",
       " '351': (-0.9876542299999997, 1.6933555874999997),\n",
       " '352': (-0.6388971339999999, 0.8714211842499999),\n",
       " '353': (-0.3368962035, 0.449194938),\n",
       " '354': (-1.8679190029999997, 2.5493066209999995),\n",
       " '355': (-2.0683382425000003, 3.0049205775000005),\n",
       " '356': (-2.0020752000000006, 3.623113950000001),\n",
       " '357': (-3.6625725449999997, 5.9014401649999995),\n",
       " '358': (-0.6374978025, 0.8499970699999999),\n",
       " '359': (-inf, inf),\n",
       " '360': (-0.49270274999999997, 0.656937),\n",
       " '361': (-inf, inf),\n",
       " '362': (-2.161469325, 7.972890225),\n",
       " '363': (-0.03365321325000001, 0.044870951000000006),\n",
       " '364': (-inf, inf),\n",
       " '365': (-2.4614861600000006, 3.6209822950000006),\n",
       " '366': (-inf, inf),\n",
       " '367': (-1.4733637700000004, 2.68161349),\n",
       " '368': (-1.973234805, 3.4812159974999997),\n",
       " '369': (-inf, inf),\n",
       " '370': (-0.6268764225000001, 0.83583523),\n",
       " '371': (-1.8429553015000004, 2.4731688692500002),\n",
       " '372': (-0.642424661, 0.96340108325),\n",
       " '373': (-0.06956455425, 0.092752739),\n",
       " '374': (-inf, inf),\n",
       " '375': (-inf, inf),\n",
       " '376': (-inf, inf),\n",
       " '377': (-0.5805584750000001, 0.7906134075000001),\n",
       " '378': (-0.6413915274, 0.8617541549250001),\n",
       " '379': (-inf, inf),\n",
       " '380': (-inf, inf),\n",
       " '381': (-0.7510974349999998, 1.3786893499999997),\n",
       " '382': (-0.5598148765, 0.8480315005),\n",
       " '383': (-0.01133908065, 0.015118774199999999),\n",
       " '384': (-0.3756264915, 0.526858368),\n",
       " '385': (-5.5707301000000005, 7.689749387500001),\n",
       " '386': (-0.0753170145, 0.100422686),\n",
       " '387': (-0.0016535836349999998, 0.00220477818),\n",
       " '388': (-0.48538567500000007, 0.6471809000000001),\n",
       " '389': (-0.24962061375000003, 0.33282748500000003),\n",
       " '390': (-inf, inf),\n",
       " '391': (-3.793225600000001, 6.528066762500001),\n",
       " '392': (-inf, inf),\n",
       " '393': (-1.152071649999999, 7.893719824999998),\n",
       " '394': (-inf, inf),\n",
       " '395': (-0.7312545060000001, 1.0774303020000002),\n",
       " '396': (-2.7856739349999997, 4.1101685324999995),\n",
       " '397': (-inf, inf),\n",
       " '398': (-inf, inf),\n",
       " '399': (-0.9912780999999999, 1.5468095324999998),\n",
       " '400': (-0.41390057249999995, 0.5518674299999999),\n",
       " '401': (-0.3591820575, 0.47890941),\n",
       " '402': (-1.5115665000000003, 7.621834175),\n",
       " '403': (-0.3744173325, 0.49922310999999997),\n",
       " '404': (-inf, inf),\n",
       " '405': (-3.001247, 5.494652650000001),\n",
       " '406': (-inf, inf),\n",
       " '407': (-inf, inf),\n",
       " '408': (-inf, inf),\n",
       " '409': (-0.32012921775000003, 0.426838957),\n",
       " '410': (-inf, inf),\n",
       " '411': (-0.04453863, 0.05938484),\n",
       " '412': (-2.9948632750000006, 8.670368625),\n",
       " '413': (-0.1388645265, 0.185152702),\n",
       " '414': (-2.396577464, 3.480256023),\n",
       " '415': (-0.00206703801, 0.0027560506800000002),\n",
       " '416': (-2.715788102, 3.6472022327499998),\n",
       " '417': (-inf, inf),\n",
       " '418': (-0.133733355, 0.17831113999999998),\n",
       " '419': (-inf, inf),\n",
       " '420': (-inf, inf),\n",
       " '421': (-inf, inf),\n",
       " '422': (-0.044654979000000004, 0.059539972000000004),\n",
       " '423': (-0.10394397825000001, 0.138591971),\n",
       " '424': (-0.08424044850000001, 0.11232059800000001),\n",
       " '425': (-2.3945478400000004, 4.021830555),\n",
       " '426': (-inf, inf),\n",
       " '427': (-0.7067317830000001, 0.99529417725),\n",
       " '428': (-0.40205659650000003, 0.55477142425),\n",
       " '429': (-inf, inf),\n",
       " '430': (-1.6456922600000006, 3.1699392075000006),\n",
       " '431': (-0.22248110625, 0.296641475),\n",
       " '432': (-5.792234109999999, 8.3212768575),\n",
       " '433': (-0.021152433749999998, 0.028203245),\n",
       " '434': (-1.6363066815, 2.4368234005000002),\n",
       " '435': (-0.3449821875, 0.45997625),\n",
       " '436': (-1.4141336749999998, 6.020703875),\n",
       " '437': (-1.1968714100000002, 1.7788010450000002),\n",
       " '438': (-1.306033085, 2.231054245),\n",
       " '439': (-1.319164374, 1.9711622705),\n",
       " '440': (-inf, inf),\n",
       " '441': (-1.6114212924999998, 2.5829741399999997),\n",
       " '442': (-0.37366750169999996, 0.500782651275),\n",
       " '443': (-0.0595960935, 0.079461458),\n",
       " '444': (-inf, inf),\n",
       " '445': (-0.0006494084175, 0.00086587789),\n",
       " '446': (-inf, inf),\n",
       " '447': (-inf, inf),\n",
       " '448': (-1.026137725, 2.9811582249999997),\n",
       " '449': (-inf, inf),\n",
       " '450': (-inf, inf),\n",
       " '451': (-2.278195875, 3.517086625),\n",
       " '452': (-inf, inf),\n",
       " '453': (-0.2825633175, 0.37675109),\n",
       " '454': (-4.411680825, 8.4594115),\n",
       " '455': (-2.19477218, 3.005894515),\n",
       " '456': (-1.780597075, 2.7291995625),\n",
       " '457': (-1.3159402055, 1.86140008475),\n",
       " '458': (-6.384719149999999, 9.781415187499999),\n",
       " '459': (-1.216316675, 3.843655375),\n",
       " '460': (-inf, inf),\n",
       " '461': (-0.44902844249999996, 0.59870459),\n",
       " '462': (-0.43686576, 0.58248768),\n",
       " '463': (-2.4033478500000003, 4.5019820500000005),\n",
       " '464': (-inf, inf),\n",
       " '465': (-0.3719519175, 0.49593589000000005),\n",
       " '466': (-inf, inf),\n",
       " '467': (-0.8291153935, 1.128370777),\n",
       " '468': (-1.1919562685, 1.6359568895),\n",
       " '469': (-6.885832390000001, 9.833590242500001),\n",
       " '470': (-0.09365913375, 0.124878845),\n",
       " '471': (-inf, inf),\n",
       " '472': (-inf, inf),\n",
       " '473': (-inf, inf),\n",
       " '474': (-0.408652138, 0.560022221625),\n",
       " '475': (-inf, inf),\n",
       " '476': (-inf, inf),\n",
       " '477': (-inf, inf),\n",
       " '478': (-inf, inf),\n",
       " '479': (-0.0127796055, 0.017039474),\n",
       " '480': (-0.06478418625, 0.086378915),\n",
       " '481': (-0.8948396494999998, 1.3070575077499997),\n",
       " '482': (-0.0207795582, 0.0277060776),\n",
       " '483': (-0.022747578, 0.030330104),\n",
       " '484': (-0.099607083, 0.132809444),\n",
       " '485': (-0.8455754624999999, 1.12743395),\n",
       " '486': (-0.0046453086, 0.0061937448),\n",
       " '487': (-0.057325113, 0.076433484),\n",
       " '488': (-0.275761305, 0.36768174000000003),\n",
       " '489': (-inf, inf),\n",
       " '490': (-0.7403814225, 0.9871752300000001),\n",
       " '491': (-inf, inf),\n",
       " '492': (-2.2199340500000004, 5.947349025),\n",
       " '493': (-inf, inf),\n",
       " '494': (-inf, inf),\n",
       " '495': (-1.0090566811000001, 1.350215148325),\n",
       " '496': (-0.03963972825, 0.052852971),\n",
       " '497': (-inf, inf),\n",
       " '498': (-0.00014330660249999998, 0.00019107547),\n",
       " '499': (-inf, inf),\n",
       " '500': (-0.4404393441000001, 0.59096926245),\n",
       " '501': (-0.92564185, 1.4254300375),\n",
       " '502': (-inf, inf),\n",
       " '503': (-3.73224183, 5.52038596),\n",
       " '504': (-1.99998521, 3.081648725),\n",
       " '505': (-0.02081365005, 0.0277515334),\n",
       " '506': (-inf, inf),\n",
       " '507': (-inf, inf),\n",
       " '508': (-2.8263613149999998, 5.3017208675),\n",
       " '509': (-0.8414998364999999, 1.1884000705),\n",
       " '510': (-inf, inf),\n",
       " '511': (-7.511147820000001, 12.1894113275),\n",
       " '512': (-3.571972795, 5.7308519775),\n",
       " '513': (-1.5999322740000002, 2.2962641980000003),\n",
       " '514': (-inf, inf),\n",
       " '515': (-2.35266363, 4.544363972499999),\n",
       " '516': (-0.0296434695, 0.039524626),\n",
       " '517': (-inf, inf),\n",
       " '518': (-0.273807063, 0.365076084),\n",
       " '519': (-1.0421891484999999, 1.4718115207499998),\n",
       " '520': (-0.20281743000000002, 0.27042324),\n",
       " '521': (-1.5158316050000002, 2.72470361),\n",
       " '522': (-4.229466124999998, 11.859005574999998),\n",
       " '523': (-inf, inf),\n",
       " '524': (-0.36752106375, 0.490028085),\n",
       " '525': (-1.5304202565000005, 2.1009962717500006),\n",
       " '526': (-inf, inf),\n",
       " '527': (-3.566409925, 5.371499999999999),\n",
       " '528': (-inf, inf),\n",
       " '529': (-0.40237697699999997, 0.58300657775),\n",
       " '530': (-0.14625740025, 0.195009867),\n",
       " '531': (-0.03144162, 0.04192216),\n",
       " '532': (-inf, inf),\n",
       " '533': (-1.4665168450000001, 2.09326416125),\n",
       " '534': (-1.005467525, 5.037711225000001),\n",
       " '535': (-2.040600525, 2.7208007),\n",
       " '536': (-inf, inf),\n",
       " '537': (-0.11714637524999999, 0.156195167),\n",
       " '538': (-0.7232158999999996, 5.942986125),\n",
       " '539': (-0.0361348875, 0.048179849999999996),\n",
       " '540': (-0.531324336, 0.7455840695),\n",
       " '541': (-0.8850911660000002, 1.2814613620000002),\n",
       " '542': (-0.10690179825, 0.142535731),\n",
       " '543': (-4.0827967500000004e-05, 5.443729e-05),\n",
       " '544': (-5.215624600000001, 8.63016855),\n",
       " '545': (-0.11156163599999999, 0.148748848),\n",
       " '546': (-inf, inf),\n",
       " '547': (-0.02106882075, 0.028091761),\n",
       " '548': (-inf, inf),\n",
       " '549': (-1.9261642700000001, 3.663317215),\n",
       " '550': (1.1042385249999995, 8.12986605),\n",
       " '551': (-inf, inf),\n",
       " '552': (-inf, inf),\n",
       " '553': (-inf, inf),\n",
       " '554': (-2.5224091000000004, 4.830818075000001),\n",
       " '555': (-inf, inf),\n",
       " '556': (-0.02685708, 0.03580944),\n",
       " '557': (-0.07224489375, 0.096326525),\n",
       " '558': (-0.1207108065, 0.160947742),\n",
       " '559': (-0.27319351724999996, 0.364258023),\n",
       " '560': (-0.10257726075000001, 0.136769681),\n",
       " '561': (-0.10601175825, 0.141349011),\n",
       " '562': (-3.04952926, 6.2844629075),\n",
       " '563': (-2.0694833999999998, 5.79968455),\n",
       " '564': (-1.5514175599999998, 2.9728849324999995),\n",
       " '565': (-0.1371752085, 0.182900278),\n",
       " '566': (-0.430134248, 0.59890273475),\n",
       " '567': (-2.1124670475, 3.1070579449999998),\n",
       " '568': (-5.94618597, 9.3889827775),\n",
       " '569': (-0.6708939675, 0.9186439124999999),\n",
       " '570': (-inf, inf),\n",
       " '571': (-inf, inf),\n",
       " '572': (-0.016691886075000002, 0.022255848100000003),\n",
       " '573': (-inf, inf),\n",
       " '574': (-inf, inf),\n",
       " '575': (-inf, inf),\n",
       " '576': (-0.5587487800499998, 0.7471377600374998),\n",
       " '577': (-inf, inf),\n",
       " '578': (-inf, inf),\n",
       " '579': (-1.867315063, 2.61501518975),\n",
       " '580': (-inf, inf),\n",
       " '581': (-1.1023271895, 1.5371059477500002),\n",
       " '582': (-inf, inf),\n",
       " '583': (-inf, inf),\n",
       " '584': (-0.26243291999999996, 0.34991056),\n",
       " '585': (-inf, inf),\n",
       " '586': (-0.34011780375, 0.453490405),\n",
       " '587': (-2.1899841825, 3.1342296775),\n",
       " '588': (-inf, inf),\n",
       " '589': (-0.05375664674999999, 0.07167552899999999),\n",
       " '590': (-0.152226195, 0.20296826),\n",
       " '591': (-inf, inf),\n",
       " '592': (-inf, inf),\n",
       " '593': (-1.6040738849999998, 4.37437457),\n",
       " '594': (-inf, inf),\n",
       " '595': (-0.09245158349999999, 0.123268778),\n",
       " '596': (-0.4555998255, 0.627605651),\n",
       " '597': (-0.0067626334500000005, 0.0090168446),\n",
       " '598': (-0.7184178375, 0.95789045),\n",
       " '599': (-1.199072645, 1.6983673987499999),\n",
       " '600': (-inf, inf),\n",
       " '601': (-0.901648556, 1.22258441325),\n",
       " '602': (-0.363477535, 0.5313471125),\n",
       " '603': (-0.2370884175, 0.31611789),\n",
       " '604': (-inf, inf),\n",
       " '605': (-2.158264705, 4.102989297500001),\n",
       " '606': (-0.9572953839999998, 1.35616467925),\n",
       " '607': (-0.0058861386, 0.0078481848),\n",
       " '608': (-inf, inf),\n",
       " '609': (-0.32086150874999997, 0.42781534499999996),\n",
       " '610': (-4.9112080549999995, 7.002434297499999),\n",
       " '611': (-inf, inf),\n",
       " '612': (-1.7923871374999998, 2.9886424375),\n",
       " '613': (-2.94162631, 4.5376071825),\n",
       " '614': (-1.5890018600000002, 2.96914432),\n",
       " '615': (-0.5934607725, 0.7912810299999999),\n",
       " '616': (-0.06657353175, 0.088764709),\n",
       " '617': (-inf, inf),\n",
       " '618': (-2.038137500000001, 3.294850300000001),\n",
       " '619': (-inf, inf),\n",
       " '620': (-inf, inf),\n",
       " '621': (-inf, inf),\n",
       " '622': (-2.9825016100000004, 5.1179862575),\n",
       " '623': (-0.005612619, 0.0074834920000000004),\n",
       " '624': (-0.004976442825, 0.0066352571),\n",
       " '625': (-0.5374175625000001, 0.7165567500000001),\n",
       " '626': (-inf, inf),\n",
       " '627': (-1.6190771000000002, 2.3905412225000005),\n",
       " '628': (-inf, inf),\n",
       " '629': (-inf, inf),\n",
       " '630': (-1.5718737999999988, 9.496688774999999),\n",
       " '631': (-1.0621006915, 1.5564058267499998),\n",
       " '632': (-0.012638233500000002, 0.016850978000000003),\n",
       " '633': (-6.933288964999999, 10.00708963),\n",
       " '634': (-1.3166402775000001, 1.7555203700000002),\n",
       " '635': (-inf, inf),\n",
       " '636': (-inf, inf),\n",
       " '637': (-0.8103299990000001, 1.1302107867500002),\n",
       " '638': (-0.768939687, 1.138675589),\n",
       " '639': (-0.006917486475, 0.0092233153),\n",
       " '640': (-inf, inf),\n",
       " '641': (-4.044922795, 5.679570689999999),\n",
       " '642': (-1.8344259275000003, 2.5276267500000005),\n",
       " '643': (-inf, inf),\n",
       " '644': (-0.81918168, 1.09224224),\n",
       " '645': (-0.8631559171, 1.1658338290749999),\n",
       " '646': (-1.0621580683000003, 1.4253424162250004),\n",
       " '647': (-inf, inf),\n",
       " '648': (-0.0015862985399999999, 0.00211506472),\n",
       " '649': (-0.6476434425, 0.86352459),\n",
       " '650': (-3.4022404749999997, 5.3053914125),\n",
       " '651': (-0.7842250179999999, 1.0568418634999999),\n",
       " '652': (1.303194650000001, 6.685960324999999),\n",
       " '653': (-inf, inf),\n",
       " '654': (-1.2766251353240001, 1.7023371233680002),\n",
       " '655': (-inf, inf),\n",
       " '656': (-inf, inf),\n",
       " '657': (-inf, inf),\n",
       " '658': (-0.017709965625, 0.0236132875),\n",
       " '659': (-inf, inf),\n",
       " '660': (-0.98657445, 1.3154326),\n",
       " '661': (-inf, inf),\n",
       " '662': (-1.0629609004999998, 1.5982406635),\n",
       " '663': (-0.146458476, 0.195277968),\n",
       " '664': (-0.1051918725, 0.14025583),\n",
       " '665': (-0.6134178214999999, 0.8596024229999999),\n",
       " '666': (-1.254758733, 1.78405498725),\n",
       " '667': (-inf, inf),\n",
       " '668': (-0.05531787149999999, 0.07375716199999999),\n",
       " '669': (-inf, inf),\n",
       " '670': (-0.1209423, 0.1612564),\n",
       " '671': (-0.039417596250000006, 0.052556795),\n",
       " '672': (-inf, inf),\n",
       " '673': (-1.1163485885, 1.6172434894999999),\n",
       " '674': (-inf, inf),\n",
       " '675': (-2.053044055, 2.957726535),\n",
       " '676': (-1.8330298000000003, 4.580727375),\n",
       " '677': (-0.230590572, 0.307454096),\n",
       " '678': (-inf, inf),\n",
       " '679': (-2.077574285, 3.3871130199999993),\n",
       " '680': (-0.211813362, 0.282417816),\n",
       " '681': (-inf, inf),\n",
       " '682': (-inf, inf),\n",
       " '683': (-0.9778175175000001, 1.3037566900000002),\n",
       " '684': (-0.9368293350000001, 1.24910578),\n",
       " '685': (-0.026223216749999997, 0.034964288999999996),\n",
       " '686': (-0.663202959, 0.8912862429999999),\n",
       " '687': (-0.5320436445, 0.76143182025),\n",
       " '688': (-inf, inf),\n",
       " '689': (-inf, inf),\n",
       " '690': (-1.307567245, 2.0485912775),\n",
       " '691': (-0.067131939, 0.089509252),\n",
       " '692': (-inf, inf),\n",
       " '693': (-inf, inf),\n",
       " '694': (-inf, inf),\n",
       " '695': (-0.5314126125, 0.70855015),\n",
       " '696': (-inf, inf),\n",
       " '697': (-3.877320465, 5.554665630000001),\n",
       " '698': (-0.4388436015, 0.604723178),\n",
       " '699': (-inf, inf),\n",
       " '700': (-0.00555900405, 0.007412005399999999),\n",
       " '701': (-inf, inf),\n",
       " '702': (-inf, inf),\n",
       " '703': (-1.2172296, 2.1127954200000003),\n",
       " '704': (-0.05721000225, 0.076280003),\n",
       " '705': (-inf, inf),\n",
       " '706': (-0.035271551250000005, 0.047028735),\n",
       " '707': (-2.266231385, 3.5735569575),\n",
       " '708': (-0.132378795, 0.17650506),\n",
       " '709': (-0.054388940999999996, 0.072518588),\n",
       " '710': (-inf, inf),\n",
       " '711': (-3.82201, 6.967484625),\n",
       " '712': (-inf, inf),\n",
       " '713': (-0.916325704, 1.2653138242500002),\n",
       " '714': (-inf, inf),\n",
       " '715': (-1.30868535, 2.1804176625),\n",
       " '716': (-0.8557587775, 1.162184335),\n",
       " '717': (-inf, inf),\n",
       " '718': (-inf, inf),\n",
       " '719': (-inf, inf),\n",
       " '720': (-inf, inf),\n",
       " '721': (-0.1953170265, 0.260422702),\n",
       " '722': (-0.5141246569999999, 0.7654657039999999),\n",
       " '723': (-1.05777951141, 1.4112344685574998),\n",
       " '724': (-0.001105661265, 0.00147421502),\n",
       " '725': (-inf, inf),\n",
       " '726': (-0.62292267, 0.83056356),\n",
       " '727': (-inf, inf),\n",
       " '728': (-1.5340476, 5.7253400249999995),\n",
       " '729': (-0.5877024525, 0.78360327),\n",
       " '730': (-inf, inf),\n",
       " '731': (-0.0392346975, 0.05231293),\n",
       " '732': (-0.9572042330000001, 1.5641731272500001),\n",
       " '733': (-inf, inf),\n",
       " '734': (-inf, inf),\n",
       " '735': (-inf, inf),\n",
       " '736': (-inf, inf),\n",
       " '737': (-1.304379925, 1.9354308537500002),\n",
       " '738': (-2.186889968, 3.0792394460000003),\n",
       " '739': (-2.0796380450000003, 3.90761354),\n",
       " '740': (-inf, inf),\n",
       " '741': (-0.01938030675, 0.025840409),\n",
       " '742': (-inf, inf),\n",
       " '743': (0.1377094999999997, 8.6835699),\n",
       " '744': (-0.7870007429999999, 1.1236818822499999),\n",
       " '745': (-2.0433691250000003, 3.8000991000000006),\n",
       " '746': (-inf, inf),\n",
       " '747': (-0.6656176455, 0.9075439322500001),\n",
       " '748': (-inf, inf),\n",
       " '749': (-0.042297720375, 0.0563969605),\n",
       " '750': (-inf, inf),\n",
       " '751': (-inf, inf),\n",
       " '752': (-0.708603225, 0.9448042999999999),\n",
       " '753': (-inf, inf),\n",
       " '754': (-inf, inf),\n",
       " '755': (-1.1119860574999998, 2.128858665),\n",
       " '756': (-inf, inf),\n",
       " '757': (-0.007172015025, 0.0095626867),\n",
       " '758': (-0.9579073080000001, 1.316283926),\n",
       " '759': (-0.7016161980000001, 0.9623698897500002),\n",
       " '760': (-1.9097443125, 2.54632575),\n",
       " '761': (-0.5092565405000001, 0.7151202460000001),\n",
       " '762': (-0.718292404, 1.109654283),\n",
       " '763': (-0.38449218000000007, 0.5126562400000001),\n",
       " '764': (-0.013818230325, 0.0184243071),\n",
       " '765': (-0.38421481700000004, 0.5359496940000001),\n",
       " '766': (-0.271793295, 0.36239106),\n",
       " '767': (-0.5585764575000001, 0.74476861),\n",
       " '768': (-inf, inf),\n",
       " '769': (-inf, inf),\n",
       " '770': (-0.056311719749999996, 0.075082293),\n",
       " '771': (-0.1389801525, 0.18530687),\n",
       " '772': (-0.15398616375000002, 0.205314885),\n",
       " '773': (-inf, inf),\n",
       " '774': (-inf, inf),\n",
       " '775': (-inf, inf),\n",
       " '776': (-1.3713951965000002, 1.89449984675),\n",
       " '777': (-0.1823238405, 0.243098454),\n",
       " '778': (-0.11571744600000002, 0.15428992800000002),\n",
       " '779': (-0.4335664875, 0.57808865),\n",
       " '780': (-0.21883034999999998, 0.29177379999999997),\n",
       " '781': (-0.020653228125, 0.0275376375),\n",
       " '782': (-0.8685303200000001, 1.2036001337500002),\n",
       " '783': (-2.5203613500000004, 6.799494125000001),\n",
       " '784': (-inf, inf),\n",
       " '785': (-0.11434768125000001, 0.15246357500000002),\n",
       " '786': (-inf, inf),\n",
       " '787': (-3.25670535, 5.3805361125),\n",
       " '788': (-0.518767049, 0.705468142375),\n",
       " '789': (-2.67213166, 4.2152007325),\n",
       " '790': (-inf, inf),\n",
       " '791': (-inf, inf),\n",
       " '792': (-inf, inf),\n",
       " '793': (-inf, inf),\n",
       " '794': (-3.018958375, 5.0519298875),\n",
       " '795': (-0.4315930125, 0.57545735),\n",
       " '796': (-0.67430874, 0.89907832),\n",
       " '797': (-inf, inf),\n",
       " '798': (-inf, inf),\n",
       " '799': (-inf, inf),\n",
       " '800': (-inf, inf),\n",
       " '801': (-0.4417132425, 0.58895099),\n",
       " '802': (-0.63631989, 0.84842652),\n",
       " '803': (-inf, inf),\n",
       " '804': (-1.5149134399999995, 2.6116628949999994),\n",
       " '805': (-0.8890275379999999, 1.2510268872499999),\n",
       " '806': (-0.022739754, 0.030319672),\n",
       " '807': (-3.3679317999999996, 10.021850375),\n",
       " '808': (-0.005098408725, 0.0067978783),\n",
       " '809': (-2.4788570749999996, 4.25674085),\n",
       " '810': (-0.46507569000000004, 0.62010092),\n",
       " '811': (-inf, inf),\n",
       " '812': (-0.3912954225, 0.52172723),\n",
       " '813': (-2.0466559650000002, 2.72887462),\n",
       " '814': (-inf, inf),\n",
       " '815': (-inf, inf),\n",
       " '816': (-2.5385020995000005, 3.5609549040000004),\n",
       " '817': (-5.69285175e-06, 7.590469e-06),\n",
       " '818': (-inf, inf),\n",
       " '819': (-inf, inf),\n",
       " '820': (-inf, inf),\n",
       " '821': (-0.9307604335, 1.27775467075),\n",
       " '822': (-1.613678775, 2.1515717),\n",
       " '823': (-inf, inf),\n",
       " '824': (-0.27208370249999997, 0.36277826999999996),\n",
       " '825': (-1.2409981575, 1.9875035524999998),\n",
       " '826': (-1.8661593849999998, 2.8167169074999996),\n",
       " '827': (-0.3672595607, 0.49249165802499995),\n",
       " '828': (-1.9979412299999995, 3.6451094349999993),\n",
       " '829': (-0.7080945504999999, 0.9749632122499998),\n",
       " '830': (-0.3691749525, 0.49223327),\n",
       " '831': (-0.07108163325, 0.094775511),\n",
       " '832': (-2.4843164210000004, 3.4863856345000004),\n",
       " '833': (-0.5965193925, 0.9162411662500001),\n",
       " '834': (-7.339751965000001, 11.323047242500001),\n",
       " '835': (-0.14423116125000002, 0.192308215),\n",
       " '836': (-3.3580104950000007, 5.525693740000001),\n",
       " '837': (-1.1661524750000023, 8.984208025000003),\n",
       " '838': (-inf, inf),\n",
       " '839': (-inf, inf),\n",
       " '840': (-0.16741589775000001, 0.223221197),\n",
       " '841': (-1.334869485, 2.443246535),\n",
       " '842': (-inf, inf),\n",
       " '843': (-inf, inf),\n",
       " '844': (-0.42218619659999995, 0.5711396887),\n",
       " '845': (-0.11362302300000002, 0.15149736400000002),\n",
       " '846': (-0.2956633275, 0.39421777),\n",
       " '847': (-inf, inf),\n",
       " '848': (-inf, inf),\n",
       " '849': (-0.5420837924999999, 0.72277839),\n",
       " '850': (-0.006687911175, 0.0089172149),\n",
       " '851': (-1.7589927100000005, 3.084553357500001),\n",
       " '852': (-0.36479737250000005, 0.4935318256250001),\n",
       " '853': (-0.19090566375, 0.254540885),\n",
       " '854': (-0.12582785925, 0.167770479),\n",
       " '855': (-1.2351483745, 1.7701550314999999),\n",
       " '856': (-inf, inf),\n",
       " '857': (-inf, inf),\n",
       " '858': (-2.7188342749999994, 6.735869199999999),\n",
       " '859': (-0.0844795275, 0.11263937),\n",
       " '860': (-0.5221272450000001, 0.69616966),\n",
       " '861': (-0.63637593, 0.84850124),\n",
       " '862': (0.14577972500000058, 6.794252324999999),\n",
       " '863': (-0.21630406350000003, 0.288405418),\n",
       " '864': (-0.664006136, 0.9176850607500001),\n",
       " '865': (-1.061643375, 1.6444089512500002),\n",
       " '866': (-0.883499835, 1.5416041575),\n",
       " '867': (-2.520948665, 4.0883387925),\n",
       " '868': (-inf, inf),\n",
       " '869': (-3.5432659029999996, 4.785602902249999),\n",
       " '870': (-2.156489395, 3.0551445150000003),\n",
       " '871': (-0.029200731375, 0.0389343085),\n",
       " '872': (-inf, inf),\n",
       " '873': (-inf, inf),\n",
       " '874': (-inf, inf),\n",
       " '875': (-0.17556073049999998, 0.234080974),\n",
       " '876': (-0.16515676875000002, 0.220209025),\n",
       " '877': (-0.9170520415000001, 1.4698644555),\n",
       " '878': (-inf, inf),\n",
       " '879': (-0.7637235645, 1.098458404),\n",
       " '880': (-0.14562005175, 0.19416006900000002),\n",
       " '881': (-0.299914725, 0.3998863),\n",
       " '882': (-inf, inf),\n",
       " '883': (-0.05464780125, 0.072863735),\n",
       " '884': (-2.67759734, 3.9169621175000002),\n",
       " '885': (-1.8308774225000002, 2.873744895),\n",
       " '886': (-inf, inf),\n",
       " '887': (-inf, inf),\n",
       " '888': (-inf, inf),\n",
       " '889': (-0.04582183725, 0.061095783),\n",
       " '890': (-0.5890789049999999, 0.7854385399999999),\n",
       " '891': (-1.0463886875, 1.65219185625),\n",
       " '892': (-1.7972150370000004, 2.5067012527500006),\n",
       " '893': (-0.0488915445, 0.065188726),\n",
       " '894': (-0.5589370874999999, 0.74524945),\n",
       " '895': (-2.3146018250000004, 5.3291407500000005),\n",
       " '896': (-inf, inf),\n",
       " '897': (-0.36592017975, 0.487893573),\n",
       " '898': (-0.042807217875, 0.0570762905),\n",
       " '899': (-inf, inf),\n",
       " '900': (-inf, inf),\n",
       " '901': (-0.038778676500000005, 0.051704902000000004),\n",
       " '902': (-inf, inf),\n",
       " '903': (-inf, inf),\n",
       " '904': (-0.12201313049999998, 0.162684174),\n",
       " '905': (-0.09709667475, 0.129462233),\n",
       " '906': (-0.6201837505, 0.9103220322499999),\n",
       " '907': (-inf, inf),\n",
       " '908': (-0.04283460075, 0.057112801),\n",
       " '909': (-0.006762422175, 0.0090165629),\n",
       " '910': (-0.03080174325, 0.041068991),\n",
       " '911': (-inf, inf),\n",
       " '912': (-inf, inf),\n",
       " '913': (-inf, inf),\n",
       " '914': (-1.224433503, 1.6639879035),\n",
       " '915': (-0.49123305000000006, 0.6549774),\n",
       " '916': (-0.147686778, 0.196915704),\n",
       " '917': (-2.63022481, 4.549181445),\n",
       " '918': (-0.6221502560000001, 0.9338466095000001),\n",
       " '919': (-inf, inf),\n",
       " '920': (-1.4495988816999998, 1.9441116687749997),\n",
       " '921': (-0.00798723165, 0.0106496422),\n",
       " '922': (-1.5751140940000004, 2.1294723317500006),\n",
       " '923': (-inf, inf),\n",
       " '924': (-2.271358483, 3.0472512435000003),\n",
       " '925': (-0.3188770725, 0.42516943),\n",
       " '926': (-1.1283542925, 1.9206605575000002),\n",
       " '927': (-2.322108775, 4.7848412499999995),\n",
       " '928': (-2.36113111, 4.1873062575),\n",
       " '929': (-0.1208960535, 0.161194738),\n",
       " '930': (-0.010215209175, 0.0136202789),\n",
       " '931': (-inf, inf),\n",
       " '932': (-0.11831215650000002, 0.15774954200000002),\n",
       " '933': (-inf, inf),\n",
       " '934': (-1.192475176, 1.8571146820000002),\n",
       " '935': (-0.026320602, 0.035094136),\n",
       " '936': (-0.21163113000000003, 0.28217484000000004),\n",
       " '937': (-1.3571836000000022, 8.037829825000003),\n",
       " '938': (-0.336446653, 0.48072625599999996),\n",
       " '939': (-1.5538847249999996, 5.0943371),\n",
       " '940': (-2.0817613, 4.63720165),\n",
       " '941': (-4.1016863, 7.2123857),\n",
       " '942': (-0.9548406406000002, 1.2788001667000002),\n",
       " '943': (-inf, inf),\n",
       " '944': (-0.5023921725, 0.6698562299999999),\n",
       " '945': (-0.51114033, 0.68152044),\n",
       " '946': (-inf, inf),\n",
       " '947': (-0.483145557, 0.6666280277500001),\n",
       " '948': (-0.11150210699999999, 0.148669476),\n",
       " '949': (-0.7842214725, 1.04562863),\n",
       " '950': (-0.9511575149999999, 1.2908947362499998),\n",
       " '951': (-inf, inf),\n",
       " '952': (-2.8748603500000005, 4.623037137500001),\n",
       " '953': (-0.02824245675, 0.037656609),\n",
       " '954': (-inf, inf),\n",
       " '955': (-2.451824775, 4.900215375),\n",
       " '956': (-0.0227782161, 0.0303709548),\n",
       " '957': (-1.4956381249999997, 3.4722829624999996),\n",
       " '958': (-0.11214498899999999, 0.14952665199999998),\n",
       " '959': (-inf, inf),\n",
       " '960': (-0.8205502025, 1.10237269625),\n",
       " '961': (-inf, inf),\n",
       " '962': (-inf, inf),\n",
       " '963': (-inf, inf),\n",
       " '964': (-inf, inf),\n",
       " '965': (-inf, inf),\n",
       " '966': (-inf, inf),\n",
       " '967': (-inf, inf),\n",
       " '968': (-inf, inf),\n",
       " '969': (-1.21321844622, 1.618643385915),\n",
       " '970': (-0.5416410975, 0.72218813),\n",
       " '971': (-inf, inf),\n",
       " '972': (-inf, inf),\n",
       " '973': (-2.6872499675, 3.7313346700000003),\n",
       " '974': (-inf, inf),\n",
       " '975': (-0.02925776175, 0.039010349),\n",
       " '976': (-0.110481033, 0.147308044),\n",
       " '977': (-inf, inf),\n",
       " '978': (-inf, inf),\n",
       " '979': (-0.3923655825, 0.52315411),\n",
       " '980': (-0.8960844035000001, 1.224296072),\n",
       " '981': (-2.0621749599999997, 3.4448178074999998),\n",
       " '982': (-inf, inf),\n",
       " '983': (-1.5632033745000002, 2.3238135215000004),\n",
       " '984': (-inf, inf),\n",
       " '985': (-1.6069109850000003, 3.0329444450000005),\n",
       " '986': (-0.5266694, 0.73416168375),\n",
       " '987': (-0.147383946, 0.196511928),\n",
       " '988': (-0.1266982095, 0.168930946),\n",
       " '989': (-inf, inf),\n",
       " '990': (-2.2391017499999997, 5.757872375),\n",
       " '991': (-0.023082230175000003, 0.0307763069),\n",
       " '992': (-inf, inf),\n",
       " '993': (-0.04497874725, 0.059971663),\n",
       " '994': (-2.7123372400000005, 4.6767248425000005),\n",
       " '995': (-0.9565618499999999, 1.2754158),\n",
       " '996': (-inf, inf),\n",
       " '997': (-0.07432337024999999, 0.099097827),\n",
       " '998': (-1.5335959999999997, 4.13035495),\n",
       " '999': (-0.031531237875, 0.0420416505),\n",
       " ...}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Guardamos el orden exacto de las columnas.\n",
    "\n",
    "#Mantener el orden es clave para que train/val/test e inferencia alimenten al modelo con los mismos features en la misma posición.\n",
    "num_cols = X.columns.tolist()\n",
    "\n",
    "# --- IQR bounds calculados SOLO en TRAIN ---\n",
    "def iqr_extreme_bounds(s: pd.Series):\n",
    "    q1 = s.quantile(0.25)\n",
    "    q3 = s.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    if pd.isna(iqr) or iqr == 0:\n",
    "        return -np.inf, np.inf\n",
    "    return float(q1 - 3.0*iqr), float(q3 + 3.0*iqr)\n",
    "\n",
    "clip_bounds = {c: iqr_extreme_bounds(X_train[c]) for c in num_cols}\n",
    "clip_bounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec10b812",
   "metadata": {},
   "source": [
    "# 2) Aplicar el clipping con los límites aprendidos\n",
    "\n",
    "Sustituimos valores extremos por el límite (winsorizing).\n",
    "\n",
    "Reduce la influencia de outliers en el entrenamiento y estabiliza la optimización del MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48ddf4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1270</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>1275</th>\n",
       "      <th>1276</th>\n",
       "      <th>1277</th>\n",
       "      <th>1278</th>\n",
       "      <th>1279</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.013247</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.061597</td>\n",
       "      <td>0.910705</td>\n",
       "      <td>0.017849</td>\n",
       "      <td>2.058289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163703</td>\n",
       "      <td>2.422755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453929</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.488104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.519049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.260857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0.142749</td>\n",
       "      <td>0.039572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041014</td>\n",
       "      <td>0.518857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146302</td>\n",
       "      <td>3.479688</td>\n",
       "      <td>...</td>\n",
       "      <td>3.249652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.742966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.058152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047652</td>\n",
       "      <td>0.034166</td>\n",
       "      <td>0.137146</td>\n",
       "      <td>0.180616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.427526</td>\n",
       "      <td>0.011019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324383</td>\n",
       "      <td>0.106006</td>\n",
       "      <td>2.137774</td>\n",
       "      <td>0.012905</td>\n",
       "      <td>0.078323</td>\n",
       "      <td>5.213752</td>\n",
       "      <td>...</td>\n",
       "      <td>1.802655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.156479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.262912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137146</td>\n",
       "      <td>1.702988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189065</td>\n",
       "      <td>0.248645</td>\n",
       "      <td>0.554901</td>\n",
       "      <td>0.172229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.057117</td>\n",
       "      <td>0.010389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.285076</td>\n",
       "      <td>...</td>\n",
       "      <td>3.324397</td>\n",
       "      <td>0.070436</td>\n",
       "      <td>1.697701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.260384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.057209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.044452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.922710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052414</td>\n",
       "      <td>2.586820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.671337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.475554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.048004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.948158</td>\n",
       "      <td>0.062166</td>\n",
       "      <td>1.113814</td>\n",
       "      <td>0.005685</td>\n",
       "      <td>0.148507</td>\n",
       "      <td>1.002321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.572262</td>\n",
       "      <td>...</td>\n",
       "      <td>1.297646</td>\n",
       "      <td>0.088397</td>\n",
       "      <td>0.485492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.177728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>1.108718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>0.197509</td>\n",
       "      <td>0.893316</td>\n",
       "      <td>0.022562</td>\n",
       "      <td>0.323131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021897</td>\n",
       "      <td>1.964728</td>\n",
       "      <td>0.043426</td>\n",
       "      <td>0.049994</td>\n",
       "      <td>1.193169</td>\n",
       "      <td>...</td>\n",
       "      <td>2.246198</td>\n",
       "      <td>0.159765</td>\n",
       "      <td>2.179276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.577255</td>\n",
       "      <td>0.020466</td>\n",
       "      <td>0.157892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>0.143083</td>\n",
       "      <td>0.008031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014213</td>\n",
       "      <td>0.290430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310832</td>\n",
       "      <td>3.421613</td>\n",
       "      <td>...</td>\n",
       "      <td>3.634339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.511030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.375237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157892</td>\n",
       "      <td>0.056456</td>\n",
       "      <td>0.137146</td>\n",
       "      <td>0.207354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.817018</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.060424</td>\n",
       "      <td>...</td>\n",
       "      <td>1.330500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.429233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.277243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009179</td>\n",
       "      <td>0.635738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.016932</td>\n",
       "      <td>2.442734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048723</td>\n",
       "      <td>0.624051</td>\n",
       "      <td>0.002886</td>\n",
       "      <td>3.601601</td>\n",
       "      <td>0.041820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.443942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.562741</td>\n",
       "      <td>0.083661</td>\n",
       "      <td>1.180304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.018061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083699</td>\n",
       "      <td>0.056456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.566799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows × 1280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "634  0.000000  2.013247  0.010597  0.061597  0.910705  0.017849  2.058289   \n",
       "274  0.142749  0.039572  0.000000  0.000000  0.000000  0.041014  0.518857   \n",
       "232  0.000000  0.427526  0.011019  0.000000  0.324383  0.106006  2.137774   \n",
       "107  0.000000  0.189065  0.248645  0.554901  0.172229  0.000000  2.057117   \n",
       "241  0.000000  1.044452  0.000000  0.000000  0.229530  0.000000  0.922710   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "699  0.000000  1.948158  0.062166  1.113814  0.005685  0.148507  1.002321   \n",
       "617  0.197509  0.893316  0.022562  0.323131  0.000000  0.021897  1.964728   \n",
       "329  0.143083  0.008031  0.000000  0.000000  0.000000  0.014213  0.290430   \n",
       "127  0.000000  0.086113  0.000000  0.000000  0.007679  0.000000  0.817018   \n",
       "209  0.016932  2.442734  0.000000  0.048723  0.624051  0.002886  3.601601   \n",
       "\n",
       "            7         8         9  ...      1270      1271      1272  1273  \\\n",
       "634  0.000000  0.163703  2.422755  ...  0.453929  0.000778  0.488104   0.0   \n",
       "274  0.000000  0.146302  3.479688  ...  3.249652  0.000000  0.742966   0.0   \n",
       "232  0.012905  0.078323  5.213752  ...  1.802655  0.000000  1.156479   0.0   \n",
       "107  0.010389  0.000000  4.285076  ...  3.324397  0.070436  1.697701   0.0   \n",
       "241  0.000000  0.052414  2.586820  ...  0.671337  0.000000  0.320128   0.0   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   ...   \n",
       "699  0.000000  0.000000  0.572262  ...  1.297646  0.088397  0.485492   0.0   \n",
       "617  0.043426  0.049994  1.193169  ...  2.246198  0.159765  2.179276   0.0   \n",
       "329  0.000000  0.310832  3.421613  ...  3.634339  0.000000  0.511030   0.0   \n",
       "127  0.000989  0.000000  4.060424  ...  1.330500  0.000000  1.429233   0.0   \n",
       "209  0.041820  0.000000  1.443942  ...  0.562741  0.083661  1.180304   0.0   \n",
       "\n",
       "         1274      1275      1276      1277      1278      1279  \n",
       "634  4.519049  0.000000  0.000000  0.001794  0.000000  2.260857  \n",
       "274  1.058152  0.000000  0.047652  0.034166  0.137146  0.180616  \n",
       "232  4.262912  0.000000  0.157892  0.000000  0.137146  1.702988  \n",
       "107  1.260384  0.000000  0.021227  0.000000  0.000000  1.057209  \n",
       "241  1.475554  0.000000  0.000000  0.008828  0.000000  1.048004  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "699  1.177728  0.000000  0.157892  0.000000  0.008397  1.108718  \n",
       "617  3.577255  0.020466  0.157892  0.000000  0.000000  0.142285  \n",
       "329  1.375237  0.000000  0.157892  0.056456  0.137146  0.207354  \n",
       "127  1.277243  0.000000  0.000000  0.000000  0.009179  0.635738  \n",
       "209  5.018061  0.000000  0.083699  0.056456  0.000000  1.566799  \n",
       "\n",
       "[154 rows x 1280 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def clip_with_bounds(df_in: pd.DataFrame, bounds: dict):\n",
    "    df_out = df_in.copy()\n",
    "    for c, (lo, hi) in bounds.items():\n",
    "        df_out[c] = df_out[c].clip(lower=lo, upper=hi)\n",
    "    return df_out\n",
    "\n",
    "X_train_clip = clip_with_bounds(X_train, clip_bounds)\n",
    "X_val_clip   = clip_with_bounds(X_val,   clip_bounds)\n",
    "X_test_clip  = clip_with_bounds(X_test,  clip_bounds)\n",
    "\n",
    "X_test_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc8157",
   "metadata": {},
   "source": [
    "# 3) Estandarización con estadísticas de TRAIN\n",
    "\n",
    "Calculamos media y desviación estándar solo en train_clippeado.\n",
    "\n",
    "Reemplazamos std=0 por 1 para evitar división por cero (columnas constantes).\n",
    "\n",
    "eps es un colchón numérico que mejora la estabilidad.\n",
    "\n",
    "Aplicamos la misma transformación a val/test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0afcef0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1270</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>1275</th>\n",
       "      <th>1276</th>\n",
       "      <th>1277</th>\n",
       "      <th>1278</th>\n",
       "      <th>1279</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>-0.585698</td>\n",
       "      <td>1.575802</td>\n",
       "      <td>-0.504013</td>\n",
       "      <td>-0.506832</td>\n",
       "      <td>2.929653</td>\n",
       "      <td>-0.382191</td>\n",
       "      <td>0.524526</td>\n",
       "      <td>-0.227211</td>\n",
       "      <td>0.151139</td>\n",
       "      <td>-0.621368</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.381131</td>\n",
       "      <td>-0.574492</td>\n",
       "      <td>-0.772759</td>\n",
       "      <td>-0.15018</td>\n",
       "      <td>2.014447</td>\n",
       "      <td>-0.053491</td>\n",
       "      <td>-0.616802</td>\n",
       "      <td>-0.474009</td>\n",
       "      <td>-0.602044</td>\n",
       "      <td>1.352213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1.820044</td>\n",
       "      <td>-0.801051</td>\n",
       "      <td>-0.656266</td>\n",
       "      <td>-0.708555</td>\n",
       "      <td>-0.627898</td>\n",
       "      <td>-0.053847</td>\n",
       "      <td>-1.296663</td>\n",
       "      <td>-0.227211</td>\n",
       "      <td>0.050005</td>\n",
       "      <td>0.273439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.933786</td>\n",
       "      <td>-0.588403</td>\n",
       "      <td>-0.398831</td>\n",
       "      <td>-0.15018</td>\n",
       "      <td>-0.887459</td>\n",
       "      <td>-0.053491</td>\n",
       "      <td>0.339546</td>\n",
       "      <td>1.165908</td>\n",
       "      <td>2.475729</td>\n",
       "      <td>-1.021839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>-0.585698</td>\n",
       "      <td>-0.333847</td>\n",
       "      <td>-0.497956</td>\n",
       "      <td>-0.708555</td>\n",
       "      <td>0.639261</td>\n",
       "      <td>0.867368</td>\n",
       "      <td>0.618558</td>\n",
       "      <td>-0.061849</td>\n",
       "      <td>-0.345082</td>\n",
       "      <td>1.741510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.264358</td>\n",
       "      <td>-0.588403</td>\n",
       "      <td>0.207867</td>\n",
       "      <td>-0.15018</td>\n",
       "      <td>1.799680</td>\n",
       "      <td>-0.053491</td>\n",
       "      <td>2.552032</td>\n",
       "      <td>-0.564916</td>\n",
       "      <td>2.475729</td>\n",
       "      <td>0.715550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>-0.585698</td>\n",
       "      <td>-0.621019</td>\n",
       "      <td>2.915971</td>\n",
       "      <td>1.108685</td>\n",
       "      <td>0.044891</td>\n",
       "      <td>-0.635182</td>\n",
       "      <td>0.523139</td>\n",
       "      <td>-0.094088</td>\n",
       "      <td>-0.800286</td>\n",
       "      <td>0.955286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995676</td>\n",
       "      <td>0.671487</td>\n",
       "      <td>1.001936</td>\n",
       "      <td>-0.15018</td>\n",
       "      <td>-0.717890</td>\n",
       "      <td>-0.053491</td>\n",
       "      <td>-0.190782</td>\n",
       "      <td>-0.564916</td>\n",
       "      <td>-0.602044</td>\n",
       "      <td>-0.021437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>-0.585698</td>\n",
       "      <td>0.409103</td>\n",
       "      <td>-0.656266</td>\n",
       "      <td>-0.708555</td>\n",
       "      <td>0.268732</td>\n",
       "      <td>-0.635182</td>\n",
       "      <td>-0.818894</td>\n",
       "      <td>-0.227211</td>\n",
       "      <td>-0.495659</td>\n",
       "      <td>-0.482470</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.201113</td>\n",
       "      <td>-0.588403</td>\n",
       "      <td>-1.019209</td>\n",
       "      <td>-0.15018</td>\n",
       "      <td>-0.537474</td>\n",
       "      <td>-0.053491</td>\n",
       "      <td>-0.616802</td>\n",
       "      <td>-0.117678</td>\n",
       "      <td>-0.602044</td>\n",
       "      <td>-0.031942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>-0.585698</td>\n",
       "      <td>1.497416</td>\n",
       "      <td>0.236866</td>\n",
       "      <td>2.939066</td>\n",
       "      <td>-0.605689</td>\n",
       "      <td>1.469777</td>\n",
       "      <td>-0.724713</td>\n",
       "      <td>-0.227211</td>\n",
       "      <td>-0.800286</td>\n",
       "      <td>-2.188009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.682516</td>\n",
       "      <td>0.992766</td>\n",
       "      <td>-0.776591</td>\n",
       "      <td>-0.15018</td>\n",
       "      <td>-0.787196</td>\n",
       "      <td>-0.053491</td>\n",
       "      <td>2.552032</td>\n",
       "      <td>-0.564916</td>\n",
       "      <td>-0.413606</td>\n",
       "      <td>0.037347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>2.742906</td>\n",
       "      <td>0.227094</td>\n",
       "      <td>-0.332128</td>\n",
       "      <td>0.349665</td>\n",
       "      <td>-0.627898</td>\n",
       "      <td>-0.324807</td>\n",
       "      <td>0.413840</td>\n",
       "      <td>0.329237</td>\n",
       "      <td>-0.509725</td>\n",
       "      <td>-1.662345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102905</td>\n",
       "      <td>2.269332</td>\n",
       "      <td>1.708491</td>\n",
       "      <td>-0.15018</td>\n",
       "      <td>1.224768</td>\n",
       "      <td>17.755992</td>\n",
       "      <td>2.552032</td>\n",
       "      <td>-0.564916</td>\n",
       "      <td>-0.602044</td>\n",
       "      <td>-1.065584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>1.825676</td>\n",
       "      <td>-0.839035</td>\n",
       "      <td>-0.656266</td>\n",
       "      <td>-0.708555</td>\n",
       "      <td>-0.627898</td>\n",
       "      <td>-0.433721</td>\n",
       "      <td>-1.566898</td>\n",
       "      <td>-0.227211</td>\n",
       "      <td>1.006240</td>\n",
       "      <td>0.224272</td>\n",
       "      <td>...</td>\n",
       "      <td>1.252315</td>\n",
       "      <td>-0.588403</td>\n",
       "      <td>-0.739122</td>\n",
       "      <td>-0.15018</td>\n",
       "      <td>-0.621588</td>\n",
       "      <td>-0.053491</td>\n",
       "      <td>2.552032</td>\n",
       "      <td>2.295123</td>\n",
       "      <td>2.475729</td>\n",
       "      <td>-0.991325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>-0.585698</td>\n",
       "      <td>-0.745003</td>\n",
       "      <td>-0.656266</td>\n",
       "      <td>-0.708555</td>\n",
       "      <td>-0.597902</td>\n",
       "      <td>-0.635182</td>\n",
       "      <td>-0.943931</td>\n",
       "      <td>-0.214539</td>\n",
       "      <td>-0.800286</td>\n",
       "      <td>0.765094</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.655312</td>\n",
       "      <td>-0.588403</td>\n",
       "      <td>0.608045</td>\n",
       "      <td>-0.15018</td>\n",
       "      <td>-0.703754</td>\n",
       "      <td>-0.053491</td>\n",
       "      <td>-0.616802</td>\n",
       "      <td>-0.564916</td>\n",
       "      <td>-0.396059</td>\n",
       "      <td>-0.502436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>-0.300352</td>\n",
       "      <td>2.093024</td>\n",
       "      <td>-0.656266</td>\n",
       "      <td>-0.548992</td>\n",
       "      <td>1.809877</td>\n",
       "      <td>-0.594273</td>\n",
       "      <td>2.350304</td>\n",
       "      <td>0.308667</td>\n",
       "      <td>-0.800286</td>\n",
       "      <td>-1.450039</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.291033</td>\n",
       "      <td>0.908044</td>\n",
       "      <td>0.242821</td>\n",
       "      <td>-0.15018</td>\n",
       "      <td>2.432860</td>\n",
       "      <td>-0.053491</td>\n",
       "      <td>1.062998</td>\n",
       "      <td>2.295123</td>\n",
       "      <td>-0.602044</td>\n",
       "      <td>0.560126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows × 1280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "634 -0.585698  1.575802 -0.504013 -0.506832  2.929653 -0.382191  0.524526   \n",
       "274  1.820044 -0.801051 -0.656266 -0.708555 -0.627898 -0.053847 -1.296663   \n",
       "232 -0.585698 -0.333847 -0.497956 -0.708555  0.639261  0.867368  0.618558   \n",
       "107 -0.585698 -0.621019  2.915971  1.108685  0.044891 -0.635182  0.523139   \n",
       "241 -0.585698  0.409103 -0.656266 -0.708555  0.268732 -0.635182 -0.818894   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "699 -0.585698  1.497416  0.236866  2.939066 -0.605689  1.469777 -0.724713   \n",
       "617  2.742906  0.227094 -0.332128  0.349665 -0.627898 -0.324807  0.413840   \n",
       "329  1.825676 -0.839035 -0.656266 -0.708555 -0.627898 -0.433721 -1.566898   \n",
       "127 -0.585698 -0.745003 -0.656266 -0.708555 -0.597902 -0.635182 -0.943931   \n",
       "209 -0.300352  2.093024 -0.656266 -0.548992  1.809877 -0.594273  2.350304   \n",
       "\n",
       "            7         8         9  ...      1270      1271      1272     1273  \\\n",
       "634 -0.227211  0.151139 -0.621368  ... -1.381131 -0.574492 -0.772759 -0.15018   \n",
       "274 -0.227211  0.050005  0.273439  ...  0.933786 -0.588403 -0.398831 -0.15018   \n",
       "232 -0.061849 -0.345082  1.741510  ... -0.264358 -0.588403  0.207867 -0.15018   \n",
       "107 -0.094088 -0.800286  0.955286  ...  0.995676  0.671487  1.001936 -0.15018   \n",
       "241 -0.227211 -0.495659 -0.482470  ... -1.201113 -0.588403 -1.019209 -0.15018   \n",
       "..        ...       ...       ...  ...       ...       ...       ...      ...   \n",
       "699 -0.227211 -0.800286 -2.188009  ... -0.682516  0.992766 -0.776591 -0.15018   \n",
       "617  0.329237 -0.509725 -1.662345  ...  0.102905  2.269332  1.708491 -0.15018   \n",
       "329 -0.227211  1.006240  0.224272  ...  1.252315 -0.588403 -0.739122 -0.15018   \n",
       "127 -0.214539 -0.800286  0.765094  ... -0.655312 -0.588403  0.608045 -0.15018   \n",
       "209  0.308667 -0.800286 -1.450039  ... -1.291033  0.908044  0.242821 -0.15018   \n",
       "\n",
       "         1274       1275      1276      1277      1278      1279  \n",
       "634  2.014447  -0.053491 -0.616802 -0.474009 -0.602044  1.352213  \n",
       "274 -0.887459  -0.053491  0.339546  1.165908  2.475729 -1.021839  \n",
       "232  1.799680  -0.053491  2.552032 -0.564916  2.475729  0.715550  \n",
       "107 -0.717890  -0.053491 -0.190782 -0.564916 -0.602044 -0.021437  \n",
       "241 -0.537474  -0.053491 -0.616802 -0.117678 -0.602044 -0.031942  \n",
       "..        ...        ...       ...       ...       ...       ...  \n",
       "699 -0.787196  -0.053491  2.552032 -0.564916 -0.413606  0.037347  \n",
       "617  1.224768  17.755992  2.552032 -0.564916 -0.602044 -1.065584  \n",
       "329 -0.621588  -0.053491  2.552032  2.295123  2.475729 -0.991325  \n",
       "127 -0.703754  -0.053491 -0.616802 -0.564916 -0.396059 -0.502436  \n",
       "209  2.432860  -0.053491  1.062998  2.295123 -0.602044  0.560126  \n",
       "\n",
       "[154 rows x 1280 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Estandarización con medias/STD de TRAIN ---\n",
    "eps   = 1e-8\n",
    "means = X_train_clip[num_cols].mean(axis=0)\n",
    "stds  = X_train_clip[num_cols].std(axis=0).replace(0.0, 1.0)\n",
    "\n",
    "def standardize(df_in: pd.DataFrame, means: pd.Series, stds: pd.Series):\n",
    "    out = df_in.copy()\n",
    "    out[num_cols] = (out[num_cols] - means) / (stds + eps)\n",
    "    return out\n",
    "\n",
    "X_train_std = standardize(X_train_clip, means, stds)\n",
    "X_val_std   = standardize(X_val_clip,   means, stds)\n",
    "X_test_std  = standardize(X_test_clip,  means, stds)\n",
    "\n",
    "X_test_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88434aa",
   "metadata": {},
   "source": [
    "# 4) Salida lista para Keras\n",
    "Convertimos a float32 (formato típico y eficiente para redes).\n",
    "\n",
    "Imprimimos shapes para confirmar coherencia dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a952fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones listas: (460, 1280) (154, 1280) (154, 1280)\n"
     ]
    }
   ],
   "source": [
    "# --- Matrices listas para Keras ---\n",
    "X_train_ready = X_train_std[num_cols].to_numpy(dtype=np.float32)\n",
    "X_val_ready   = X_val_std[num_cols].to_numpy(dtype=np.float32)\n",
    "X_test_ready  = X_test_std[num_cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "y_train_np = y_train.to_numpy(dtype=int)\n",
    "y_val_np   = y_val.to_numpy(dtype=int)\n",
    "y_test_np  = y_test.to_numpy(dtype=int)\n",
    "\n",
    "print(\"Dimensiones listas:\", X_train_ready.shape, X_val_ready.shape, X_test_ready.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cf9f2f",
   "metadata": {},
   "source": [
    "# Beneficios y consideraciones (concluciones)\n",
    "\n",
    "* Sin fuga de información: todo (umbrales, medias, std) se aprende en TRAIN y se aplica a VAL/TEST.\n",
    "\n",
    "* Robustez: el clipping con ±3×IQR atenúa valores aberrantes que desestabilizan el MLP.\n",
    "\n",
    "* Estabilidad numérica: std.replace(0,1) y eps evitan problemas en columnas constantes o con muy baja varianza.\n",
    "\n",
    "* Reproducible: si fijas SEED y usas el mismo flujo, obtendrás los mismos splits/transformaciones.\n",
    "\n",
    "* Alternativas si quieres experimentar:\n",
    "\n",
    "    - RobustScaler (mediana/IQR) en lugar de Z-score.\n",
    "\n",
    "    - QuantileTransformer o PowerTransformer si hay colas muy pesadas.\n",
    "\n",
    "    - Cambiar el umbral de clipping (±2.5×IQR o percentiles 1–99).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e663c4fe",
   "metadata": {},
   "source": [
    "# pesos de clase por desbalance\n",
    "\n",
    "Con 78.5% vs 21.5% puede ayudar a estabilizar entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f07f44e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6371191135734072, 1: 2.323232323232323}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim  = X_train_ready.shape[1]\n",
    "n_classes  = int(np.unique(y_train_np).size)\n",
    "\n",
    "classes = np.unique(y_train_np)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=y_train_np\n",
    ")\n",
    "class_weight_dict = {int(c): float(w) for c, w in zip(classes, class_weights)}\n",
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71fad53",
   "metadata": {},
   "source": [
    "# Reproducibilidad\n",
    "\n",
    "Fijar semillas reduce la variabilidad entre corridas (mismo split + mismo init ≈ mismas curvas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e434ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab718606",
   "metadata": {},
   "source": [
    "# Constructor de los modelos utilizando keras\n",
    "\n",
    "Qué hace: construye y compila un MLP en Keras a partir de un diccionario de hiperparámetros cfg.\n",
    "\n",
    "Arquitectura mínima pero potente (2 capas ReLU), regularización combinada (L2 + Dropout), optimizador moderno (AdamW) y una loss adecuada a etiquetas enteras. Todo parametrizado vía cfg para reusar en entrenamiento base y tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "048cfe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_cfg(cfg, input_dim, n_classes):\n",
    "    \"\"\"Crea el MLP a partir de un diccionario cfg.\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(cfg[\"hidden1\"], activation=\"relu\",\n",
    "                     kernel_regularizer=regularizers.l2(cfg[\"wd\"])),\n",
    "        layers.Dropout(cfg[\"drop1\"]),\n",
    "        layers.Dense(cfg[\"hidden2\"], activation=\"relu\",\n",
    "                     kernel_regularizer=regularizers.l2(cfg[\"wd\"])),\n",
    "        layers.Dropout(cfg[\"drop2\"]),\n",
    "        layers.Dense(n_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "    opt = optimizers.AdamW(learning_rate=cfg[\"lr\"], weight_decay=cfg[\"wd\"])\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5dd7e7",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "\n",
    "* EarlyStopping: corta entrenamiento cuando deja de mejorar (y restaura el mejor estado).\n",
    "\n",
    "* ReduceLROnPlateau: baja el LR para escapar mesetas sin reiniciar entrenamiento.\n",
    "\n",
    "* ModelCheckpoint (opcional): guarda en disco el mejor modelo durante el training\n",
    "\n",
    "* Qué hace: crea los callbacks que controlan el entrenamiento.\n",
    "\n",
    "* En síntesis: control automático del entrenamiento para evitar sobreentrenar y ajustar LR cuando se estanca, sin intervención manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b358096",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_callbacks(pat_es=15, pat_rlr=6, monitor=\"val_loss\"):\n",
    "    \"\"\"Devuelve EarlyStopping y ReduceLROnPlateau configurados.\"\"\"\n",
    "    es  = callbacks.EarlyStopping(monitor=monitor, patience=pat_es,\n",
    "                                  restore_best_weights=True, verbose=0)\n",
    "    rlr = callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.5,\n",
    "                                      patience=pat_rlr, min_lr=1e-6, verbose=0)\n",
    "    return [es, rlr]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20941da1",
   "metadata": {},
   "source": [
    "# Entrenador\n",
    "* Entrena con validación en cada época, class_weight para el desbalance y callbacks para control del proceso.\n",
    "\n",
    "* Qué hace: entrena el modelo y calcula un conjunto de métricas de validación relevantes (incluyendo métricas para desbalance).\n",
    "\n",
    "* Binario vs. multiclase:\n",
    "\n",
    "    - Binario: usa val_probs[:, 1] como prob. de la clase positiva → ROC-AUC y Average Precision (PR-AUC) bien definidas.\n",
    "\n",
    "    - Multiclase: calcula AUC-ROC macro (ovo) y AP macro; promedia el rendimiento en todas las clases.\n",
    "\n",
    "* Métricas devueltas:\n",
    "\n",
    "    - val_loss: model.evaluate(...)[0]. Criterio principal de selección (umbral-independiente y estable).\n",
    "    - val_acc: exactitud general (útil, pero puede ser engañosa con desbalance).\n",
    "    - val_f1: balancea precision y recall; binary para 0/1 o macro si multiclase.\n",
    "    - val_auc: calidad de ranking (probabilidades); clave con desbalance.\n",
    "    - val_ap: área bajo la curva precisión–recobrado; muy informativa para clase minoritaria.\n",
    "    - epochs_ran: número de épocas efectivas (diagnóstico de early stopping y ritmo de convergencia).\n",
    "\n",
    "* Por qué este set de métricas:\n",
    "\n",
    "    - val_loss para seleccionar modelos de manera robusta.\n",
    "    - F1 / AUC / AP porque tu problema está desbalanceado; miden mejor la utilidad práctica que la accuracy sola.\n",
    "    - Mantener probabilidades permite inspección posterior (umbralización distinta, curvas, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9054557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_eval(model, Xtr, ytr, Xva, yva, batch, epochs, class_weight, cbs):\n",
    "    \"\"\"Entrena y evalúa en validación. Devuelve (history, metrics_dict).\"\"\"\n",
    "    hist = model.fit(\n",
    "        Xtr, ytr,\n",
    "        validation_data=(Xva, yva),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch,\n",
    "        class_weight=class_weight,\n",
    "        callbacks=cbs,\n",
    "        verbose=0,\n",
    "    )\n",
    "    # predicciones y métricas val\n",
    "    val_probs = model.predict(Xva, verbose=0)\n",
    "    val_pred  = val_probs.argmax(axis=1)\n",
    "\n",
    "    # binario vs multiclase\n",
    "    if val_probs.shape[1] == 2:\n",
    "        val_p1 = val_probs[:, 1]\n",
    "        val_auc = roc_auc_score(yva, val_p1)\n",
    "        val_ap  = average_precision_score(yva, val_p1)\n",
    "        avg = \"binary\"\n",
    "    else:\n",
    "        val_auc = roc_auc_score(yva, val_probs, multi_class=\"ovo\", average=\"macro\")\n",
    "        val_ap  = average_precision_score(yva, val_probs, average=\"macro\")\n",
    "        avg = \"macro\"\n",
    "\n",
    "    metrics = {\n",
    "        \"val_loss\": float(model.evaluate(Xva, yva, verbose=0)[0]),\n",
    "        \"val_acc\":  float(accuracy_score(yva, val_pred)),\n",
    "        \"val_f1\":   float(f1_score(yva, val_pred, average=avg)),\n",
    "        \"val_auc\":  float(val_auc),\n",
    "        \"val_ap\":   float(val_ap),\n",
    "        \"epochs_ran\": len(hist.history.get(\"loss\", [])),\n",
    "    }\n",
    "    return hist, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8bb37c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(hist, smooth_k=None, show_acc=True, figsize=(6,4)):\n",
    "    \"\"\"\n",
    "    Grafica curvas de entrenamiento a partir de un History de Keras.\n",
    "    - smooth_k: tamaño de ventana para suavizar (media móvil). None = sin suavizado.\n",
    "    - show_acc: si True, intenta graficar accuracy (si existe en el history).\n",
    "    \"\"\"\n",
    "\n",
    "    def smooth(x, k):\n",
    "        if k is None or k <= 1 or len(x) < k: \n",
    "            return x\n",
    "        w = np.ones(k)/k\n",
    "        return np.convolve(x, w, mode=\"valid\")\n",
    "\n",
    "    H = hist.history\n",
    "    epochs = range(1, len(H.get(\"loss\", [])) + 1)\n",
    "\n",
    "    # --- Loss ---\n",
    "    train_loss = H.get(\"loss\", [])\n",
    "    val_loss   = H.get(\"val_loss\", [])\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(epochs[:len(smooth(train_loss, smooth_k))] if smooth_k else epochs,\n",
    "             smooth(train_loss, smooth_k), label=\"Train loss\")\n",
    "    if val_loss:\n",
    "        plt.plot(epochs[:len(smooth(val_loss, smooth_k))] if smooth_k else epochs,\n",
    "                 smooth(val_loss, smooth_k), label=\"Val loss\")\n",
    "    plt.xlabel(\"Época\"); plt.ylabel(\"Pérdida (loss)\")\n",
    "    ttl = \"Curva de pérdida por época\"\n",
    "    if smooth_k and smooth_k > 1: ttl += f\" (smooth k={smooth_k})\"\n",
    "    plt.title(ttl); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # --- Accuracy (si existe y si show_acc=True) ---\n",
    "    if show_acc:\n",
    "        train_acc = H.get(\"accuracy\", []) or H.get(\"acc\", [])\n",
    "        val_acc   = H.get(\"val_accuracy\", []) or H.get(\"val_acc\", [])\n",
    "        if train_acc:\n",
    "            plt.figure(figsize=figsize)\n",
    "            plt.plot(epochs[:len(smooth(train_acc, smooth_k))] if smooth_k else epochs,\n",
    "                     smooth(train_acc, smooth_k), label=\"Train acc\")\n",
    "            if val_acc:\n",
    "                plt.plot(epochs[:len(smooth(val_acc, smooth_k))] if smooth_k else epochs,\n",
    "                         smooth(val_acc, smooth_k), label=\"Val acc\")\n",
    "            plt.xlabel(\"Época\"); plt.ylabel(\"Accuracy\")\n",
    "            ttl = \"Curva de accuracy por época\"\n",
    "            if smooth_k and smooth_k > 1: ttl += f\" (smooth k={smooth_k})\"\n",
    "            plt.title(ttl); plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cc5624",
   "metadata": {},
   "source": [
    "# Entrenamiento “base”\n",
    "\n",
    "* Definir una configuración base de hiperparámetros\n",
    "    - Capas densas de 512 → 256 neuronas: capacidad suficiente para tabular con muchos features.\n",
    "    - Dropout 0.2 + L2 (wd=1e-4): regularización combinada para frenar overfitting.\n",
    "    - AdamW, lr=1e-3: tasa de aprendizaje segura/estándar; weight decay coherente con la L2 de las capas.\n",
    "    - batch=128: equilibrio típico entre estabilidad y velocidad.\n",
    "    - epochs=200 con EarlyStopping (pat_es=15) y ReduceLROnPlateau (pat_rlr=6): entrenas “hasta que deje de mejorar” sin quedarte corto ni sobreentrenar.\n",
    "* Construiste el modelo con esa config\n",
    "    - Creacion del modelo base utilizando el helper build_mlp_cfg para crear el MLP (misma arquitectura y optimizador, pero parametrizados con BASE_CFG).\n",
    "* Crear los callbacks con esa config\n",
    "    - EarlyStopping + ReduceLROnPlateau con las paciencias definidas en BASE_CFG.\n",
    "* Entrenar y Evaluar en validación, devolviendo métricas\n",
    "    - Entrenas con class_weight (corrige el desbalance 78/22).\n",
    "    - train_and_eval te devuelve:\n",
    "        - hist_base: historia de entrenamiento (loss/accuracy por época).\n",
    "        - metrics_base: val_loss, val_acc, val_f1, val_auc, val_ap, epochs_ran.\n",
    "* Imprimir las métricas base\n",
    "    - Da un punto de referencia (baseline) antes del tuning. Sirve para saber si el tuning realmente mejora.\n",
    "\n",
    "# Concluciones\n",
    "\n",
    "* Baseline sólido y reproducible: una config razonable y estable para comparar contra las variantes del tuning.\n",
    "* Reuso sin duplicación: usas build_mlp_cfg, make_callbacks, train_and_eval para que el entrenamiento base y el tuning compartan exactamente la misma lógica.\n",
    "* Regularización balanceada (Dropout + L2 + AdamW) con control del proceso (EarlyStopping + ReduceLR) → menor riesgo de overfitting en tabular con muchas columnas.\n",
    "* Métricas adecuadas para desbalance (además de accuracy, miras F1, ROC-AUC y PR-AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6b9ffaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas base (val): {'val_loss': 0.3842492699623108, 'val_acc': 0.9285714285714286, 'val_f1': 0.8405797101449275, 'val_auc': 0.9679439018281994, 'val_ap': 0.8773932776283819, 'epochs_ran': 102}\n"
     ]
    }
   ],
   "source": [
    "# Hiperparámetros base (tu config original)\n",
    "BASE_CFG = {\n",
    "    \"hidden1\": 512, \"hidden2\": 256,\n",
    "    \"drop1\": 0.2, \"drop2\": 0.2,\n",
    "    \"lr\": 1e-3, \"wd\": 1e-4,\n",
    "    \"batch\": 128, \"epochs\": 200,\n",
    "    \"pat_es\": 15, \"pat_rlr\": 6,\n",
    "}\n",
    "\n",
    "model_base = build_mlp_cfg(BASE_CFG, input_dim, n_classes)\n",
    "cbs_base   = make_callbacks(BASE_CFG[\"pat_es\"], BASE_CFG[\"pat_rlr\"])\n",
    "hist_base, metrics_base = train_and_eval(\n",
    "    model_base,\n",
    "    X_train_ready, y_train_np,\n",
    "    X_val_ready,   y_val_np,\n",
    "    batch=BASE_CFG[\"batch\"], epochs=BASE_CFG[\"epochs\"],\n",
    "    class_weight=class_weight_dict, cbs=cbs_base\n",
    ")\n",
    "print(\"Métricas base (val):\", metrics_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202d127",
   "metadata": {},
   "source": [
    "# Tuning ligero (random search) sin repetir nada\n",
    "\n",
    "* nicialización del muestreador\n",
    "    - Qué: Crea un generador de números aleatorios con semilla fija.\n",
    "    - Por qué: Hace el tuning reproducible: si vuelves a correr, obtienes las mismas configuraciones en el mismo orden.\n",
    "\n",
    "* Espacio de búsqueda y nº de pruebas\n",
    "    - Qué: Defines rangos discretos para cada hiperparámetro:\n",
    "        - H1/H2: tamaño de las dos capas densas.\n",
    "        - DR: dropout por capa (controla overfitting).\n",
    "        - LR: learning rate de AdamW.\n",
    "        - WD: weight decay (L2).\n",
    "        - BS: batch size.\n",
    "        - N_TRIALS: nº de configuraciones a evaluar.\n",
    "    - Por qué: Rangos razonables para tabular; un random search pequeño (12) cubre el espacio sin costo excesivo (80/20).\n",
    "* Muestreo de una configuración\n",
    "    - Qué: Toma una combinación al azar de cada lista y fija también épocas y “paciencias”.\n",
    "    - Por qué: Mantiene constantes los “timers” (EarlyStopping/ReduceLR) y explora lo más sensible: capacidad, regularización, LR y batch.\n",
    "* ucle de experimentos\n",
    "    - Qué: Para cada “trial”:\n",
    "        -Construye el modelo con la misma función que usas en entrenamiento normal → nada de duplicación.\n",
    "        -Crea los callbacks con las paciencias de la config.\n",
    "        -Entrena y evalúa en validación con train_and_eval, que ya computa val_loss, val_acc, val_f1, val_auc, val_ap.\n",
    "    - Por qué: Garantiza que tuning y entrenamiento base usan idéntico pipeline (justo lo que querías: reusar helpers).\n",
    "* Registro de resultados y selección del mejor\n",
    "    - Qué:\n",
    "        - Guarda hiperparámetros + métricas de cada trial en results.\n",
    "        - Actualiza best si la val_loss mejora.\n",
    "    - Por qué:\n",
    "        - Trazabilidad: podrás comparar, ordenar, justificar la elección.\n",
    "        - Criterio robusto: val_loss es menos dependiente de umbrales y estable; si te interesa priorizar minoritaria, puedes cambiar el criterio a val_f1/val_ap/val_auc.\n",
    "* Tabla ordenada y reporte\n",
    "    - Qué:\n",
    "        - Crea un DataFrame, lo ordena por val_loss y muestra el Top 5 más la mejor configuración y sus métricas.\n",
    "    - Por qué:\n",
    "        - Vista rápida para decidir si te quedas con la ganadora por val_loss o prefieres otra con mejor F1/AP (según objetivos del taller)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19c7ce27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 por val_loss:\n",
      " trial  hidden1  hidden2  drop1  drop2    lr      wd  batch  val_loss  val_acc   val_f1  val_auc   val_ap  epochs_ran\n",
      "     5      256      256    0.3    0.1 0.001 0.00001    256  0.256771 0.915584 0.805970 0.957175 0.869036          26\n",
      "     0      256      256    0.2    0.2 0.001 0.00030     64  0.288793 0.928571 0.835821 0.977711 0.918034         100\n",
      "     7      512      256    0.1    0.2 0.001 0.00010     64  0.312911 0.922078 0.823529 0.967944 0.909147          20\n",
      "     3      256      128    0.3    0.3 0.001 0.00010    256  0.315237 0.889610 0.738462 0.950413 0.837062          23\n",
      "    11      256      256    0.3    0.3 0.001 0.00001    256  0.348760 0.883117 0.727273 0.942149 0.862279          39\n",
      "\n",
      "Mejor cfg: {'hidden1': 256, 'hidden2': 256, 'drop1': 0.3, 'drop2': 0.1, 'lr': 0.001, 'wd': 1e-05, 'batch': 256, 'epochs': 200, 'pat_es': 15, 'pat_rlr': 6}\n",
      "Métricas val: {'val_loss': 0.2567710280418396, 'val_acc': 0.9155844155844156, 'val_f1': 0.8059701492537313, 'val_auc': 0.9571750563486101, 'val_ap': 0.869035794179924, 'epochs_ran': 26}\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Espacio de búsqueda\n",
    "H1, H2 = [256, 512], [128, 256]\n",
    "DR, LR, WD, BS = [0.1, 0.2, 0.3], [3e-4, 1e-3, 3e-3], [1e-5, 1e-4, 3e-4], [64, 128, 256]\n",
    "N_TRIALS = 12\n",
    "\n",
    "def sample_cfg():\n",
    "    return {\n",
    "        \"hidden1\": int(rng.choice(H1)),\n",
    "        \"hidden2\": int(rng.choice(H2)),\n",
    "        \"drop1\":   float(rng.choice(DR)),\n",
    "        \"drop2\":   float(rng.choice(DR)),\n",
    "        \"lr\":      float(rng.choice(LR)),\n",
    "        \"wd\":      float(rng.choice(WD)),\n",
    "        \"batch\":   int(rng.choice(BS)),\n",
    "        \"epochs\":  200,\n",
    "        \"pat_es\":  15, \"pat_rlr\": 6,\n",
    "    }\n",
    "\n",
    "results, best = [], None\n",
    "\n",
    "for t in range(N_TRIALS):\n",
    "    cfg = sample_cfg()\n",
    "    model_t = build_mlp_cfg(cfg, input_dim, n_classes)\n",
    "    cbs     = make_callbacks(cfg[\"pat_es\"], cfg[\"pat_rlr\"])\n",
    "    hist_t, m = train_and_eval(\n",
    "        model_t, X_train_ready, y_train_np,\n",
    "        X_val_ready, y_val_np,\n",
    "        batch=cfg[\"batch\"], epochs=cfg[\"epochs\"],\n",
    "        class_weight=class_weight_dict, cbs=cbs\n",
    "    )\n",
    "\n",
    "    row = {\"trial\": t, **cfg, **m}\n",
    "    results.append(row)\n",
    "    if (best is None) or (m[\"val_loss\"] < best[\"metrics\"][\"val_loss\"]):\n",
    "        best = {\"trial\": t, \"cfg\": cfg, \"metrics\": m, \"model\": model_t, \"history\": hist_t}\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values(\"val_loss\").reset_index(drop=True)\n",
    "print(\"\\nTop 5 por val_loss:\")\n",
    "print(res_df[[\"trial\",\"hidden1\",\"hidden2\",\"drop1\",\"drop2\",\"lr\",\"wd\",\"batch\",\n",
    "              \"val_loss\",\"val_acc\",\"val_f1\",\"val_auc\",\"val_ap\",\"epochs_ran\"]].head(5).to_string(index=False))\n",
    "\n",
    "print(\"\\nMejor cfg:\", best[\"cfg\"])\n",
    "print(\"Métricas val:\", best[\"metrics\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0850b4",
   "metadata": {},
   "source": [
    "# Entrenamiento final (con la mejor cfg)\n",
    "* Unir TRAIN + VAL para el entrenamiento final\n",
    "    - Aprovechas todos los datos disponibles para ajustar los pesos finales (salvo TEST).\n",
    "* Usar la mejor config del tuning\n",
    "* Entrenar\n",
    "* Evaluar en TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bf0aa1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from None to 1.91176, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 2: val_loss improved from 1.91176 to 0.29230, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 0.29230 to 0.12711, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 4: val_loss improved from 0.12711 to 0.10019, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.10019\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.10019\n",
      "\n",
      "Epoch 7: val_loss improved from 0.10019 to 0.09844, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 8: val_loss improved from 0.09844 to 0.05146, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 0.05146 to 0.02923, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 10: val_loss improved from 0.02923 to 0.02145, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 11: val_loss improved from 0.02145 to 0.01844, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 12: val_loss improved from 0.01844 to 0.01803, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.01803\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.01803\n",
      "\n",
      "Epoch 15: val_loss improved from 0.01803 to 0.01551, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 16: val_loss improved from 0.01551 to 0.01220, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 17: val_loss improved from 0.01220 to 0.01017, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 18: val_loss improved from 0.01017 to 0.00902, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 19: val_loss improved from 0.00902 to 0.00857, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 20: val_loss improved from 0.00857 to 0.00830, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 21: val_loss improved from 0.00830 to 0.00815, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 22: val_loss improved from 0.00815 to 0.00806, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 23: val_loss improved from 0.00806 to 0.00799, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 24: val_loss improved from 0.00799 to 0.00794, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 25: val_loss improved from 0.00794 to 0.00792, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 26: val_loss improved from 0.00792 to 0.00791, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 27: val_loss improved from 0.00791 to 0.00787, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 28: val_loss improved from 0.00787 to 0.00783, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 29: val_loss improved from 0.00783 to 0.00779, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 30: val_loss improved from 0.00779 to 0.00769, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 31: val_loss improved from 0.00769 to 0.00762, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 32: val_loss improved from 0.00762 to 0.00757, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 33: val_loss improved from 0.00757 to 0.00750, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 34: val_loss improved from 0.00750 to 0.00745, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 35: val_loss improved from 0.00745 to 0.00742, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 36: val_loss improved from 0.00742 to 0.00738, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 37: val_loss improved from 0.00738 to 0.00732, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 38: val_loss improved from 0.00732 to 0.00729, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 39: val_loss improved from 0.00729 to 0.00727, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 40: val_loss improved from 0.00727 to 0.00724, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 41: val_loss improved from 0.00724 to 0.00722, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 42: val_loss improved from 0.00722 to 0.00719, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 43: val_loss improved from 0.00719 to 0.00717, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 44: val_loss improved from 0.00717 to 0.00713, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 45: val_loss improved from 0.00713 to 0.00711, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 46: val_loss improved from 0.00711 to 0.00709, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 47: val_loss improved from 0.00709 to 0.00708, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 48: val_loss improved from 0.00708 to 0.00706, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 49: val_loss improved from 0.00706 to 0.00704, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 50: val_loss improved from 0.00704 to 0.00703, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 51: val_loss improved from 0.00703 to 0.00702, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 52: val_loss improved from 0.00702 to 0.00701, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 53: val_loss improved from 0.00701 to 0.00701, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.00701\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.00701\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.00701\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.00701\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.00701\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.00701\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.00701\n",
      "\n",
      "Epoch 61: val_loss improved from 0.00701 to 0.00701, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 62: val_loss improved from 0.00701 to 0.00701, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 63: val_loss improved from 0.00701 to 0.00701, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 64: val_loss improved from 0.00701 to 0.00700, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 65: val_loss improved from 0.00700 to 0.00700, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 66: val_loss improved from 0.00700 to 0.00700, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 67: val_loss improved from 0.00700 to 0.00700, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 68: val_loss improved from 0.00700 to 0.00700, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 69: val_loss improved from 0.00700 to 0.00699, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 70: val_loss improved from 0.00699 to 0.00699, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 71: val_loss improved from 0.00699 to 0.00699, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 72: val_loss improved from 0.00699 to 0.00699, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 73: val_loss improved from 0.00699 to 0.00699, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 74: val_loss improved from 0.00699 to 0.00699, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 75: val_loss improved from 0.00699 to 0.00699, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 76: val_loss improved from 0.00699 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 77: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 78: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 79: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 80: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 81: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 82: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 83: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 84: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 85: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 86: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 87: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 88: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 89: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 90: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 91: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.00698\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.00698\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.00698\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.00698\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.00698\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00698\n",
      "\n",
      "Epoch 98: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 99: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 100: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 101: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 102: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 103: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 104: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 105: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 106: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 107: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 108: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 109: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 110: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 111: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 112: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 113: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 114: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 115: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 116: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 117: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 118: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 119: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 120: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 121: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 122: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 123: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 124: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 125: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 126: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 127: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 128: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 129: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 130: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 131: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 132: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 133: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 134: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 135: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 136: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 137: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 138: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 139: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 140: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 141: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 142: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 143: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 144: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 145: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 146: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 147: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 148: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 149: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 150: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 151: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 152: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 153: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 154: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 155: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 156: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 157: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 158: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 159: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 160: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 161: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 162: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 163: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 164: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 165: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 166: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.00698\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.00698\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.00698\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.00698\n",
      "\n",
      "Epoch 171: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 172: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 173: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 174: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 175: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 176: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 177: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 178: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 179: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 180: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 181: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 182: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 183: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 184: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 185: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 186: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 187: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 188: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 189: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 190: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 191: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 192: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 193: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 194: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 195: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 196: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 197: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 198: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 199: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "\n",
      "Epoch 200: val_loss improved from 0.00698 to 0.00698, saving model to models/mlp_best.keras\n",
      "[TEST] loss=0.5688 acc=0.9481\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1) Combinar TRAIN + VAL para entrenar final (sigues usando val para ES/monitor) ---\n",
    "X_trfinal = np.vstack([X_train_ready, X_val_ready])\n",
    "y_trfinal = np.concatenate([y_train_np, y_val_np])\n",
    "\n",
    "final_cfg   = best[\"cfg\"]  # o BASE_CFG si prefieres\n",
    "model_final = build_mlp_cfg(final_cfg, input_dim, n_classes)\n",
    "\n",
    "# --- 2) Callback para guardar el \"mejor\" según val_loss ---\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Opción A: guardar TODO el modelo (recomendado para re-cargar sin reconstruir)\n",
    "best_model_path = \"models/mlp_best.keras\"\n",
    "ckpt = ModelCheckpoint(\n",
    "    filepath=best_model_path,\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,   # guarda modelo completo\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# (si prefieres SOLO pesos, usa:)\n",
    "# best_weights_path = \"models/mlp_best.weights.h5\"\n",
    "# ckpt = ModelCheckpoint(\n",
    "#     filepath=best_weights_path,\n",
    "#     monitor=\"val_loss\",\n",
    "#     save_best_only=True,\n",
    "#     save_weights_only=True,   # guarda solo pesos\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# --- 3) Callbacks finales: EarlyStopping/ReduceLR + checkpoint ---\n",
    "cbs_final = make_callbacks(final_cfg[\"pat_es\"], final_cfg[\"pat_rlr\"])\n",
    "cbs_final = cbs_final + [ckpt]\n",
    "\n",
    "# --- 4) Entrenamiento final ---\n",
    "hist_final, _ = train_and_eval(\n",
    "    model_final, X_trfinal, y_trfinal,\n",
    "    X_val_ready, y_val_np,             # validación para ES/monitor\n",
    "    batch=final_cfg[\"batch\"], epochs=final_cfg[\"epochs\"],\n",
    "    class_weight=class_weight_dict, cbs=cbs_final\n",
    ")\n",
    "\n",
    "# --- 5) Cargar el MEJOR estado antes de evaluar en TEST ---\n",
    "# Si guardaste modelo completo:\n",
    "model_final = load_model(best_model_path)\n",
    "\n",
    "# (Si guardaste solo pesos, entonces:)\n",
    "# model_final.load_weights(best_weights_path)\n",
    "\n",
    "# --- 6) Evaluación en TEST + curvas ---\n",
    "test_loss, test_acc = model_final.evaluate(X_test_ready, y_test_np, verbose=0)\n",
    "print(f\"[TEST] loss={test_loss:.4f} acc={test_acc:.4f}\")\n",
    "\n",
    "plot_history(hist_final, smooth_k=5, show_acc=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb2ff9",
   "metadata": {},
   "source": [
    "# Ploteo de metiricas\n",
    "* Elegir el modelo a evaluar\n",
    "    - Qué:\n",
    "        - apuntas al modelo sobre el que quieres medir desempeño en TEST.\n",
    "    - Por qué:\n",
    "        - si ya reentrenaste con TRAIN+VAL, lo lógico es evaluar model_final. Si aún no, best[\"model\"] (el mejor del tuning) también sirve.\n",
    "    \n",
    "* Probabilidades y clases predichas en TEST\n",
    "    - Qué:\n",
    "        - obtienes probabilidades por clase (softmax) y luego la clase predicha con argmax.\n",
    "    - Por qué:\n",
    "        - muchas métricas (ROC-AUC, PR-AUC) necesitan probabilidades; para la matriz de confusión necesitas las clases discretas.\n",
    "*Matriz de confusión\n",
    "    - Qué: creas la matriz [filas = verdad, columnas = predicho] en el orden fijo [0, 1].\n",
    "    - Por qué: fija el orden de etiquetas evita confusiones y hace la matriz comparable entre corridas.\n",
    "        - cm[0,0]: verdaderos negativos (TN)\n",
    "        - cm[0,1]: falsos positivos (FP)\n",
    "        - cm[1,0]: falsos negativos (FN)\n",
    "        - cm[1,1]: verdaderos positivos (VP)\n",
    "* Matriz normalizada por filas\n",
    "    - Qué:\n",
    "        - normalizas cada fila para que la suma sea 1.\n",
    "    - Por qué:\n",
    "        - cada fila representa los casos reales de una clase; al normalizar por filas obtienes el recall/sensibilidad por clase (qué % de una clase real fue bien identificado).\n",
    "* Gráfico de la matriz de confusión\n",
    "    - Qué y por qué:\n",
    "        - plt.imshow(cm): muestra la matriz como heatmap simple para leer patrones visualmente (dónde se concentra el error).\n",
    "        - xticks/yticks: etiquetas claras para ejes (Pred vs True) y clases (0,1).\n",
    "        - plt.text(...): pone el conteo exacto en cada celda → lectura inmediata sin calcular mentalmente.\n",
    "        - tight_layout(): evita que los textos se corten.\n",
    "        - show(): renderiza la figura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ddf9c58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión (conteos) [rows=verdad, cols=pred]:\n",
      " [[120   1]\n",
      " [  7  26]]\n",
      "\n",
      "Matriz de confusión normalizada por filas (recall por clase):\n",
      " [[0.992 0.008]\n",
      " [0.212 0.788]]\n",
      "\n",
      "Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.945     0.992     0.968       121\n",
      "           1      0.963     0.788     0.867        33\n",
      "\n",
      "    accuracy                          0.948       154\n",
      "   macro avg      0.954     0.890     0.917       154\n",
      "weighted avg      0.949     0.948     0.946       154\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# usa el modelo final; si no lo tienes, puedes usar best[\"model\"]\n",
    "mdl = model_final  # o: mdl = best[\"model\"]\n",
    "\n",
    "# predicción en TEST\n",
    "test_probs = mdl.predict(X_test_ready, verbose=0)\n",
    "y_test_pred = np.argmax(test_probs, axis=1)\n",
    "\n",
    "# matriz de confusión (conteos)\n",
    "cm = confusion_matrix(y_test_np, y_test_pred, labels=[0,1])\n",
    "print(\"Matriz de confusión (conteos) [rows=verdad, cols=pred]:\\n\", cm)\n",
    "\n",
    "# matriz normalizada por filas (recalls)\n",
    "cm_norm = confusion_matrix(y_test_np, y_test_pred, labels=[0,1], normalize=\"true\")\n",
    "print(\"\\nMatriz de confusión normalizada por filas (recall por clase):\\n\", np.round(cm_norm, 3))\n",
    "\n",
    "# reporte completo por si lo necesitas\n",
    "print(\"\\nClassification report (TEST):\")\n",
    "print(classification_report(y_test_np, y_test_pred, digits=3))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(cm, interpolation=\"nearest\")\n",
    "plt.title(\"Matriz de confusión (TEST)\")\n",
    "plt.xticks([0,1], [\"Pred 0\",\"Pred 1\"])\n",
    "plt.yticks([0,1], [\"True 0\",\"True 1\"])\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "plt.xlabel(\"Predicho\"); plt.ylabel(\"Real\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8b5c32",
   "metadata": {},
   "source": [
    "# Qué hicimos (Concluciones)\n",
    "\n",
    "* Montamos un random search pequeño sobre hiperparámetros clave del MLP (anchos de capas, dropout, LR, weight decay y batch size).\n",
    "\n",
    "* Entrenamos cada configuración con EarlyStopping y ReduceLROnPlateau, usando class_weight para compensar el desbalance 78/22.\n",
    "\n",
    "* Seleccionamos el mejor modelo por val_loss y registramos métricas complementarias (accuracy, F1, ROC-AUC, PR-AUC) en validación.\n",
    "\n",
    "# Beneficios\n",
    "\n",
    "* Eficiencia: pocas corridas bien elegidas → mejoras reales sin gastar tiempo excesivo (principio de 80/20).\n",
    "\n",
    "* Robustez: EarlyStopping + ReduceLROnPlateau evitan sobreentrenar y ajustan el LR de forma adaptativa.\n",
    "\n",
    "* Mejor sensibilidad a la minoritaria: class_weight reduce el sesgo hacia la clase mayoritaria; junto a F1/PR-AUC permite evaluar utilidad real.\n",
    "\n",
    "* Sin fuga de información: tuning sólo con train/val; el test queda intacto para estimar generalización al final.\n",
    "\n",
    "* Trazabilidad: res_df conserva hiperparámetros y métricas por trial → fácil comparar y justificar la elección.\n",
    "\n",
    "# Lo que revelan las métricas (cómo leerlas)\n",
    "\n",
    "* val_loss: criterio principal estable y menos dependiente del umbral; minimiza error global.\n",
    "\n",
    "* val_auc y val_ap: capturan ranking y calidad en clase positiva; útiles con desbalance.\n",
    "\n",
    "* val_f1: balancea precision/recall; si sube cuando accuracy no, indica mejor trato de la minoritaria.\n",
    "\n",
    "* epochs_ran: si la mayoría corta temprano, el modelo puede estar sobre-capacitado o el LR alto; si corre muchas épocas sin bajar, quizá hace falta más capacidad o LR más alto.\n",
    "\n",
    "# Consideraciones / riesgos\n",
    "\n",
    "* Sobreajuste de hiperparámetros a la validación: por eso dejamos test virgen para el informe final.\n",
    "\n",
    "* Variabilidad estocástica: con redes pequeñas, la semilla importa; si dos configs son muy cercanas, puedes re-correr la mejor 2–3 veces para confirmar.\n",
    "\n",
    "* LR/WD extremos: LR muy alto puede oscilar; WD muy alto puede infra-ajustar. Mantenerlos en rangos razonables (p. ej., LR ∈ [3e-4, 3e-3], WD ∈ [1e-5, 3e-4]) es prudente.\n",
    "\n",
    "* Dropout excesivo: >0.4 en tabular suele empeorar; 0.1–0.3 es un “sweet spot” típico."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
