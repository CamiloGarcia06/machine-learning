# CÃ³mo ejecutar el notebook del Taller 3 (local, Docker/Compose/Make o Colab)

Este README explica **cÃ³mo ejecutar el notebook existente** con todo el pipeline (limpieza, clustering, perfilado y anÃ¡lisis de consumo), **instalar dependencias** y una **breve guÃ­a de la estructura interna del notebook**.  
> No agrega contenidos nuevos: solo indica cÃ³mo **correr** lo que ya estÃ¡.

---

## ğŸ“‚ Supuestos mÃ­nimos del repo

- Un notebook principal, por ejemplo: `notebooks/Taller3.ipynb` *(si el nombre/ruta es otra, ajÃºstalo en los comandos)*.
- El dataset **no se versiona**: debes tener `data/drug_consumption.data` (UCI ML).  
- Ya existen `Dockerfile` y `docker-compose.yml` en la raÃ­z del proyecto. *(Solo los usamos)*
- (Opcional) `Makefile` con atajos (si ya lo tienes).

---

## âš™ï¸ Dependencias (si corres **local/venv**)
Python **3.10+** (recomendado 3.11) y:

```
numpy>=1.26
pandas>=2.1
scikit-learn>=1.4
scipy>=1.11
matplotlib>=3.8
joblib>=1.3
jupyterlab>=4.0
```

InstalaciÃ³n rÃ¡pida (venv):
```bash
python -m venv .venv
# Activar
source .venv/bin/activate          # Linux/Mac
# .venv\Scripts\activate           # Windows PowerShell
# Instalar
pip install -U pip
pip install -r requirements.txt    # si existe
# o bien:
pip install numpy pandas scikit-learn scipy matplotlib joblib jupyterlab
```

---

## ğŸš€ EjecuciÃ³n **local (venv)**

1) Coloca el dataset en `data/drug_consumption.data`.  
2) Lanza Jupyter:
```bash
jupyter lab
```
3) Abre `notebooks/Taller3.ipynb` y ejecuta todas las celdas (o por secciones).

> **Ruta del dataset**: el notebook incluye (o puedes usar) un snippet que busca `drug_consumption.data` en rutas tÃ­picas. Si falla, edita la variable `DATA_PATH` con la ruta absoluta.

---

## ğŸ³ EjecuciÃ³n con **Docker / Docker Compose / Make**

### A) Docker (con Dockerfile existente)
Si tu `Makefile` ya define atajos, Ãºsalos. Por ejemplo:
```bash
make d.build        # construye la imagen
make d.nb           # levanta Jupyter Lab (expone 8888 por defecto)
```
Luego abre `http://localhost:8888` y carga `notebooks/Taller3.ipynb`.

> Si no usas Make:  
> ```bash
> docker build -t taller3:latest .
> docker run --rm -p 8888:8888 -v "$PWD":/work -w /work taller3:latest \
>   jupyter lab --ip=0.0.0.0 --NotebookApp.token= --NotebookApp.password=
> ```

### B) Docker Compose (con `docker-compose.yml` existente)
Si tu Makefile lo envuelve:
```bash
make d.up      # docker compose up -d
make d.down    # docker compose down
```
O directamente:
```bash
docker compose up -d
# ... abre http://localhost:8888
docker compose down
```

> AsegÃºrate de que el volumen mapee el repo (para que el contenedor vea `notebooks/` y `data/`).

---

## ğŸ§ª EjecuciÃ³n en **Google Colab**

1) Abre Colab y sube el notebook `Taller3.ipynb` (o Ã¡brelo desde Drive/GitHub).  
2) Instala dependencias mÃ­nimas:
```python
!pip -q install numpy pandas scikit-learn scipy matplotlib joblib
```
3) Sube el dataset o monta Drive:
```python
# OpciÃ³n A: subir manualmente
from google.colab import files
_ = files.upload()  # selecciona drug_consumption.data

# OpciÃ³n B: montar Drive
from google.colab import drive
drive.mount('/content/drive')
```
4) Ajusta la ruta del dataset (`DATA_PATH`) si no estÃ¡ en la misma carpeta del notebook (por ejemplo `/content/drug_consumption.data`).  
5) Ejecuta el notebook por completo o secciÃ³n a secciÃ³n.

---

## ğŸ§­ Estructura del notebook (guÃ­a rÃ¡pida)

El notebook estÃ¡ organizado (o se recomienda) en estos bloques:

1. **Setup & Carga de datos**  
   - BÃºsqueda robusta del archivo `drug_consumption.data`.  
   - AsignaciÃ³n de nombres de columna (32 columnas).  
2. **Limpieza**  
   - NormalizaciÃ³n de etiquetas CL0â€“CL6 en las 19 drogas.  
   - ConversiÃ³n de rasgos psicomÃ©tricos y socio-demogrÃ¡ficos a numÃ©ricos.  
   - Chequeos bÃ¡sicos de valores faltantes/outliers.
3. **SelecciÃ³n de features (X)**  
   - Solo variables **psicomÃ©tricas y sociodemogrÃ¡ficas** (edad, gÃ©nero, educaciÃ³n, paÃ­s, etnia, N/E/O/A/C, Impulsiveness, SS).  
4. **EstandarizaciÃ³n**  
   - `StandardScaler` sobre X.  
5. **PCA exploratoria** *(solo visualizaciÃ³n)*  
   - Varianza explicada y dispersiÃ³n PC1/PC2 coloreada por clÃºster.  
6. **SelecciÃ³n de K**  
   - K=2â€¦10 con K-Means/Ward: **Elbow (inercia)**, **Silhouette**, **Calinskiâ€“Harabasz**, **Daviesâ€“Bouldin**.  
7. **Entrenamiento final**  
   - K elegido, `random_state=42`, `n_init=50` (K-Means).  
   - Guardado de **pipeline/labels/metadatos** en `final_artifacts/`.  
8. **Perfilado de clÃºsteres**  
   - Medias/medianas/z-scores por feature y **heatmap**.  
   - **Silhouette plot** y **PCA 2D** coloreado por clÃºster.  
9. **RelaciÃ³n clÃºster â†” consumo (19 drogas)**  
   - BinarizaciÃ³n *ever/recent/frequent*.  
   - Prevalencias %, **Ï‡Â²**, **V de CramÃ©r**, BH-FDR.  
   - **Heatmap** y **barras apiladas**.  
10. **Sensibilidad y validaciÃ³n**  
   - Cambiar esquema de binarizaciÃ³n y K alternativo; ARI/NMI/Jaccard/Ï.  
   - DetecciÃ³n de clÃºsteres pequeÃ±os.
11. **Export de salidas**  
   - Tablas y figuras a carpetas: `eda_outputs/`, `profiling_outputs/`, `cluster_drug_relation_outputs/`, `sensitivity_outputs/`.

> Si algÃºn bloque no aplica a tu versiÃ³n del notebook, ignÃ³ralo; la idea es darte el mapa general.

---

## ğŸ§° Problemas comunes

- **FileNotFoundError** del dataset â†’ verifica que `data/drug_consumption.data` exista o ajusta `DATA_PATH`.  
- **`Series.append` (pandas â‰¥2.0)** â†’ usa `pd.concat` o construye dicts y crea `DataFrame`.  
- **Ward no tiene `.predict()`** â†’ para inferencia, asigna al **centroide estandarizado mÃ¡s cercano**.  
- **K distintos entre particiones** â†’ reporta **NMI/ARI** sin alinear; alinear solo para comparar descripciones por clÃºster base.

---

## ğŸ“¤ Salidas principales
- `final_artifacts/` â€” pipeline(s) entrenados, labels y metadatos.  
- `profiling_outputs/` â€” z-scores/medias/medianas/figuras.  
- `cluster_drug_relation_outputs/` â€” prevalencias, Ï‡Â², V de CramÃ©r, BH.  
- `sensitivity_outputs/` â€” comparativas por esquema y K alternativo.

---
