# Taller 3 â€” Clustering en variables psico+sociodemogrÃ¡ficas (Drug Consumption UCI)

Este repo contiene el **pipeline completo** para clusterizar individuos usando **Ãºnicamente** variables
psicomÃ©tricas y sociodemogrÃ¡ficas y, sobre los clÃºsteres resultantes, analizar **patrones de consumo**
para 19 sustancias (binarizado segÃºn *ever / recent / frequent*).

Funciona en **Google Colab**, **local** (venv) o con **Docker** usando atajos del **Makefile**.

> Dataset: `drug_consumption.data` (UCI ML). No se versiona; colÃ³calo en la ruta indicada abajo.

---

## ğŸ§­ Estructura sugerida del proyecto

```
.
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 00_setup.ipynb
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â”œâ”€â”€ 02_cleaning.ipynb
â”‚   â”œâ”€â”€ 03_k_selection.ipynb
â”‚   â”œâ”€â”€ 04_stability.ipynb
â”‚   â”œâ”€â”€ 05_final_training.ipynb
â”‚   â”œâ”€â”€ 06_cluster_profiling.ipynb
â”‚   â””â”€â”€ 07_cluster_vs_consumption.ipynb
â”œâ”€â”€ data/
â”‚   â””â”€â”€ drug_consumption.data         # <-- coloca aquÃ­ el archivo (no versionado)
â”œâ”€â”€ src/                              # (opcional) helpers .py si los separas del notebook
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Makefile
â”œâ”€â”€ docker-compose.yml (opcional)
â””â”€â”€ README.md
```

### Carpetas de salidas (se crean en ejecuciÃ³n)
- `eda_outputs/`
- `k_selection_outputs/`
- `final_artifacts/` (pipeline K-Means/Ward, labels, metadatos)
- `profiling_outputs/`
- `cluster_drug_relation_outputs/`
- `sensitivity_outputs/<timestamp>/`

---

## ğŸ“¦ Dependencias

Python **3.10+** (recomendado 3.11). Paquetes clave:
- `numpy`, `pandas`, `scikit-learn`, `scipy`, `matplotlib`, `joblib`, `jupyterlab`

Archivo sugerido `requirements.txt`:
```
numpy>=1.26
pandas>=2.1
scikit-learn>=1.4
scipy>=1.11
matplotlib>=3.8
joblib>=1.3
jupyterlab>=4.0
```

---

## ğŸš€ Uso en **Google Colab**

1) **Abrir** `notebooks/00_setup.ipynb` en Colab o crear uno nuevo.
2) **Instalar** dependencias mÃ­nimas:
   ```python
   !pip -q install numpy pandas scikit-learn scipy matplotlib joblib
   ```
3) **Subir** el dataset o montar Drive:
   ```python
   # OpciÃ³n A: subir manual
   from google.colab import files
   up = files.upload()  # selecciona drug_consumption.data

   # OpciÃ³n B: usar Google Drive
   from google.colab import drive
   drive.mount('/content/drive')

   # Ruta a tu dataset
   DATA_PATH = "/content/drug_consumption.data"   # o en tu Drive
   ```
4) **Cargar datos** (snippet robusto de bÃºsqueda):
   ```python
   from pathlib import Path
   import pandas as pd

   NAME = "drug_consumption.data"
   CANDIDATES = [Path.cwd()/NAME, Path("/content")/NAME, Path("data")/NAME, Path.home()/"Downloads"/NAME]

   found = next((p for p in CANDIDATES if p.is_file()), None)
   if not found:
       hits = list(Path.cwd().rglob(NAME))
       found = hits[0] if hits else None
   if not found:
       raise FileNotFoundError("Coloca el dataset junto al notebook o define DATA_PATH manualmente.")
   DATA_PATH = found

   COLS = ["id","age","gender","education","country","ethnicity",
           "Nscore","Escore","Oscore","Ascore","Cscore","impulsive","SS",
           "alcohol","amphet","amyl","benzos","caff","cannabis","choc","coke",
           "crack","ecstasy","heroin","ketamine","legalh","lsd","meth","mushrooms",
           "nicotine","semer","vsa"]
   df = pd.read_csv(DATA_PATH, header=None, names=COLS)
   df.head()
   ```
5) ContinÃºa con los notebooks **01 â†’ 07** (EDA, limpieza, selecciÃ³n de K, estabilidad, entrenamiento final,
   perfilado y relaciÃ³n clÃºsterâ†”consumo).

---

## ğŸ’» Uso **local** (venv)

```bash
# 1) crear entorno
python -m venv .venv
# 2) activar
source .venv/bin/activate        # Linux/Mac
# .venv\Scripts\activate         # Windows
# 3) instalar deps
pip install -U pip
pip install -r requirements.txt
# 4) ejecutar jupyter lab
jupyter lab
```
Coloca `data/drug_consumption.data` y abre los notebooks en `notebooks/`.

---

## ğŸ³ Uso con **Docker** + **Makefile**

### Requisitos
- Docker / Docker Desktop
- (opcional) Docker Compose
- GNU Make

### Atajos del **Makefile** (sugeridos)
> Si ya tienes un Makefile, verifica que los targets coincidan; si no, puedes adoptar estos:

```makefile
# --- Config ---
IMG ?= taller3:latest
NB_PORT ?= 8888
CONTAINER ?= taller3-nb

# --- Docker build ---
d.build: ## Construye la imagen
\tdocker build -t $(IMG) -f Dockerfile .

# --- Jupyter (docker run) ---
d.nb: ## Lanza Jupyter Lab (token vacÃ­o) en el puerto NB_PORT
\tdocker run --name $(CONTAINER) --rm -p $(NB_PORT):8888 \
\t  -v $(PWD):/work -w /work $(IMG) \
\t  jupyter lab --ip=0.0.0.0 --NotebookApp.token='' --NotebookApp.password=''

d.exec: ## Abre una shell dentro del contenedor en ejecuciÃ³n
\tdocker exec -it $(CONTAINER) bash

d.stop: ## Detiene el contenedor de Jupyter
\t-@docker stop $(CONTAINER)

# --- Compose (opcional) ---
d.up: ## Levanta stack con docker-compose
\tdocker compose up -d

d.down: ## Baja el stack
\tdocker compose down

.PHONY: d.build d.nb d.exec d.stop d.up d.down
```

### Dockerfile mÃ­nimo (sugerido)
Si no tienes uno, un ejemplo bÃ¡sico:
```dockerfile
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    numpy pandas scikit-learn scipy matplotlib joblib jupyterlab

WORKDIR /work
EXPOSE 8888

CMD ["jupyter", "lab", "--ip=0.0.0.0", "--NotebookApp.token=", "--NotebookApp.password="]
```

### Comandos Ãºtiles
```bash
make d.build        # construir imagen
make d.nb           # iniciar Jupyter en http://localhost:8888
make d.exec         # entrar al contenedor
make d.stop         # detener Jupyter (docker run)
make d.up           # (si usas docker-compose) levantar
make d.down         # (si usas docker-compose) bajar
```

> Coloca el dataset en `data/drug_consumption.data` o mÃ³ntalo desde tu host. El volumen `-v $(PWD):/work` ya sincroniza el repo.

---

## ğŸ” Flujo de trabajo (resumen rÃ¡pido)

1. **Limpieza**: normaliza CL0â€“CL6, asegura tipos numÃ©ricos (N,E,O,A,C, Impulsiveness, SS, age, gender, education, country, ethnicity).  
2. **EstandarizaciÃ³n**: `StandardScaler` sobre features psico+socio.  
3. **SelecciÃ³n de K**: K=2â€¦10 con **K-Means** y **Ward** usando **elbow**, **silhouette**, **CH**, **DB**.  
4. **Estabilidad**: bootstrap y perturbaciÃ³n (ARI/NMI).  
5. **Entrenamiento final**: fija `random_state=42`, `n_init=50` (K-Means). Guarda **pipeline**, **labels** y **metadatos**.  
6. **Perfilado**: medias/medianas por clÃºster, **z-scores**, heatmap, PCA 2D, silhouette plot.  
7. **RelaciÃ³n consumo**: binarizaciÃ³n (*ever/recent/frequent*), prevalencias por clÃºster, **Ï‡Â²**, **V de CramÃ©r**, BH-FDR, heatmap y barras apiladas.  
8. **Sensibilidad/validaciÃ³n**: cambia esquema, prueba **K alternativo**, detecta clÃºsteres <3â€“5%.

Las salidas se guardan en las carpetas mencionadas arriba para facilitar el **informe final**.

---

## ğŸ”’ Reproducibilidad

- Semillas: usa `random_state=42` y `n_init=50` en K-Means.  
- Serializa artefactos:  
  - `final_artifacts/kmeans_pipeline.joblib`, `labels_kmeans.csv`, `kmeans_meta.json`  
  - (Ward) `ward_scaler.joblib`, `ward_centroids_scaled.npy`, `labels_ward.csv`, `ward_meta.json`  
- Incluye versiones en los metadatos (`numpy/pandas/sklearn`).

---

## ğŸ§° Troubleshooting

- **FileNotFoundError (dataset)**: verifica ruta `data/drug_consumption.data` o define `DATA_PATH` manualmente.  
- **Pandas â‰¥2.0**: `Series.append` fue eliminado â†’ usa `pd.concat` o construye dicts/Series y `DataFrame`.  
- **Ward sin `.predict()`**: asigna por **centroide mÃ¡s cercano** en espacio estandarizado.  
- **K distintos** al comparar particiones: reporta **NMI/ARI** sin alinear; alinear solo para descripciones por clÃºster base.

---

## ğŸ“‘ Licencia y cita de datos
El dataset proviene de UCI Machine Learning Repository. Consulta y respeta su licencia/cita oficial.
Este repo es solo con fines acadÃ©micos.

---

Â¡Listo! Si quieres, puedo adaptar este README a la estructura exacta de tu repo (nombres de carpeta, servicios del `docker-compose`, etc.).
